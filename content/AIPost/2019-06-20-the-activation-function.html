---
title: The Activation Function
author: andrea perlato
date: '2019-06-20'
slug: the-activation-function
categories:
  - artificial intelligence
tags:
  - activation function
---



<style>
body {
text-align: justify}
</style>
<p>What an <strong>artifical neuron</strong> do is to calculate a <strong>weighted sum</strong> of its input, adds a bias and then decides whether it should be “fired” or not.</p>
<center>
<img src="/img/artificialneuron.png" style="width:30.0%" />
</center>
<p>considering the neuron of the figure above, the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. How do we decide whether the neuron should fire or not? We use <strong>activatioin functions</strong> for this purpose. To check the Y value produced by a neuron and decide whether outside connections should consider this neuron as fired or not.</p>
<center>
<img src="/img/activatioinfunction.png" style="width:60.0%" />
</center>
<p>Generally, we have four different types of activation function that we can choose from. Of course, there are more different types, but these four are the predominate ones. </br> The <a href="https://en.wikipedia.org/wiki/Activation_function"><strong>Threshold Function</strong></a> the the simplest function where if the value is less than zero then the function is zero, but if the value is more thae zero the function is one. It is basically a yes/no type ofo function.</p>
<center>
<img src="/img/thresholdfunction.png" style="width:40.0%" />
</center>
<p>The <a href="https://en.wikipedia.org/wiki/Activation_function"><strong>Sigmoid Function</strong></a> the value <strong>x</strong> in its formula is the value of the sum of the weighed. It is a function which is used in the <strong>logistic regression</strong>. It has a gradual progression and anything below zero is just zero, and above zero it approximates to one. It is expecially useful in the output layer, especially when we are tring to predict probabilities</p>
<center>
<div class="figure">
<img src="/img/sigmoidfunction.png" style="width:40.0%" />

</div>
</center>
<p>The <a href="https://en.wikipedia.org/wiki/Activation_function"><strong>Rectifier Function</strong></a> is one of the most popular functions for artificial neural networks so when its goes all the way to zero it is zero, and from there it is fradually progresses as the input value increase as well.</p>
<center>
<img src="/img/rectifierfunction.png" style="width:40.0%" />
</center>
<p>The <a href="https://en.wikipedia.org/wiki/Activation_function"><strong>Hyperbolic Tangent tanh Function</strong></a> is very similar to the sigmoid function, but here the hyperbolic tangent function goes below zero, and that could be useful in some application such as</p>
<center>
<img src="/img/hypebolicfunctio.png" style="width:40.0%" />
</center>
<p>Now, considering for example to have an 0/1 outpu, we can use as activation function the threshold function or the sigmoid function. The advantage of the sigmoid function is that we have the probability of the output to be yes or no.</p>
<center>
<img src="/img/exampleactivationfunction.png" style="width:60.0%" />
</center>
<p>The sigmoid activation function tells us the probability of the output to be equal to one. </br> What is commonly used is to apply the <strong>rectifier activation function</strong> for the <strong>hidden layer</strong> and then the signals are passed on to the <strong>output layer</strong> where the <strong>sigmoid activation function</strong> is used, and that will be the final output that predict the probability.</p>
