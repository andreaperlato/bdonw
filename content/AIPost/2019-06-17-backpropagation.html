---
title: Backpropagation Intuition
author: andrea perlato
date: '2019-06-17'
categories:
  - artificial intelligence
tags:
  - backpropagation
slug: backpropagation
---



<style>
body {
text-align: justify}
</style>
<p>The basic idea of backpropagation is to guess what the hidden units should look like based on what the input looks like and what the output should look like. The definition of backpropagation is a way of computing gradients through recursive application of chain rules. It is the standard way of computing gradients for ANNs. It is a very flexible solution for comupting the gradient through simple, incremental steps. </br> The first step is to understand what the <a href="https://en.wikipedia.org/wiki/Backpropagation"><strong>Chain Rule</strong></a> is all about. </br></p>
<p><strong>Chain Rule</strong> </br> The fundamental unit of every deep learning model is the Perceptron. The Perceptron algorithm was designed to classify visual inputs, categorizing subjects into one of two types and separating groups with a line: it is used for linearly-solvable problems. </br></p>
<p>In calculus, the Chain rule is a formula for computing the derivative of the composition of two or more functions called <a href="https://en.wikipedia.org/wiki/Perceptron"><strong>Perceptron</strong></a>. The Chain Rule allow us to break down a complex function in many smaller functions for which it is relatively straighforward to compute the derivative. Then, we simply chain them together by multiplication. </br></p>
<p>Let’s assume we have a set of neurons with weght as described by the figure below. </br></p>
<center>
<img src="/img/backpropagationweights.png" style="width:40.0%" />
</center>
<p>We can apply an <strong>Activation Function</strong> in order to have a smoother ranging from 0 to 1, for example the <strong>Sigmoid Function</strong> described by the formula below. </br></p>
<center>
<img src="/img/activationfunctionsigmoid.png" style="width:30.0%" />
</center>
<p>We want to compute the derivative with respect of the weight <strong>w0</strong> till to <strong>wn</strong>. First of all, we semplify the function introducing the <strong>g</strong> tems.</p>
<center>
<img src="/img/derivativeactivationfunction.jpg" style="width:35.0%" />
</center>
<p>Once we have the <strong>Gradient of the Loss Function</strong> with respect to <strong>All the Weights</strong> we can update our model simply by the function below, that make a subtraction of the gradient by alfa times which is the the learinign rate. Here, for semplicity, is not consider the <strong>Regularization term</strong>, but we have to remember to always take it into consideration, especially for large models. Of course, we don’t have to go through all this math each time we want to develop a new deep learning model, but we can use the fully-fledged packages.</p>
<p><strong>Hyper-parameters Optimization</strong> </br> Tillnow, we have seen how to Learn the Weights of the Perceptron on any Deep Learning Models. There are possibility to automatic tuning the hyper-parameters especially using <a href="https://en.wikipedia.org/wiki/Random_search"><strong>Random Search</strong></a> and <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization"><strong>Grid Search</strong></a>.</p>
<p>In the following example, we consider to tuning the hyper-parameter using the <a href="https://andrea-perlato.netlify.com/aipost/stochastic-gradient-descent/"><strong>Stochastic Gradient Descent</strong></a>. In general, evaluating the Loss Function with respect to the hyper-parameters means training the model from scratch each time which makes the problem computationally impossible to solve. That is why, it is pretty common to tune the hyper-parameters manually with some tricks such as <strong>Grid Search</strong>, <strong>Random Search</strong>, and <strong>Bayesian Methods</strong>.</p>
<center>
<img src="/img/gridrandomtuning.png" style="width:50.0%" />
</center>
<p>Let’s say we want to tune just two hyper-parameters, and each of the boxes represented by the figure above, represent the hyper-parameter space in which we can choose the hyper-parameter. </br> Since we can not test each possible combinations of th two hyper-parameters we can decide to cover the space with <strong>Discrete</strong> values in <strong>Grid Search</strong>, and <strong>Random Pairs</strong> in <strong>Random Search</strong>. It turns out that in general the Random Layout is better than the Grid Layout because for the hyper-parameters we can test more different values for the Important Parameters. </br> More pratically, we can implement a Random Search in R using <a href="https://en.wikipedia.org/wiki/H2O_(software)"><strong>H2O</strong></a> which come with a complete set of utility functions specifically designed for this purpose. The <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html"><strong>h2o.grid()</strong></a> function is very general and adapttive an helps to implement the Grid or Random Search with few lines of R code even for a comple hyper-parameters selection. </br></p>
<p>In the following example we use the Wisconsin Breast-Cancer Dataset which is a collectioin of Dr.Wolberg real clinical cases. There are no images, but we can recognize malignal tumor based on 10 biomedical attributes. We have a total number of 699 patients divided in two classes: malignal and benign cancer.</p>
<pre class="r"><code># Load libraries
library(mlbench)
library(h2o)

# General parameters
h2o.init()</code></pre>
<pre><code> Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         12 minutes 22 seconds 
    H2O cluster timezone:       Europe/Berlin 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.20.0.8 
    H2O cluster version age:    8 months and 27 days !!! 
    H2O cluster name:           H2O_started_from_R_perlatoa_dpi487 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   2.31 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  4 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         AutoML, Algos, Core V3, Core V4 
    R Version:                  R version 3.5.1 (2018-07-02) </code></pre>
<pre class="r"><code>set.seed(1)

# Load the data
data(BreastCancer)

# Convert data for h2o
data &lt;- BreastCancer[, -1] # remove ID
data[, c(1:ncol(data))] &lt;- sapply(data[, c(1:ncol(data))], as.numeric) # interpret each features as numeric
data[, &#39;Class&#39;] &lt;- as.factor(data[, &#39;Class&#39;]) # interpret dependent variable as factor

# convert the dataset in three part in the h2o format
splitSample &lt;- sample(1:3, size=nrow(data), prob=c(0.7,0.15,0.15), replace=TRUE)
train_h2o &lt;- as.h2o(data[splitSample==1,])</code></pre>
<pre><code>
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>valid_h2o &lt;- as.h2o(data[splitSample==2,])</code></pre>
<pre><code>
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>test_h2o &lt;- as.h2o(data[splitSample==3,])</code></pre>
<pre><code>
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<p>Now, we can choose the best hyper-parameters using the <strong>Random Search</strong>. As we can see from the code below, we try to use many <strong>Activation Functions</strong>, many <strong>Hiden Layers with different units</strong>, different <strong>Drop-out Ratio</strong>, and <strong>L1 and L2 Regularization</strong>.</p>
<pre class="r"><code># Set hyper-parameters
hyper_params &lt;- list(
  activation =c(&quot;Rectifier&quot;, &quot;Tanh&quot;, &quot;Maxout&quot;,
                &quot;RectifierWithDropout&quot;, &quot;TanhWithDropout&quot;, &quot;MaxoutWithDropout&quot;), # different activation function
                hidden = list(c(20,20), c(50,50), c(30,30,30), c(25,25,25,25)), # different layers with different units
                input_dropout_ratio = c(0, 0.05), # different drop-out ratio
                l1 = seq(0, 1e-4, 1e-6), # L1 regularization
                l2 = seq(0, 1e-4, 1e-6)) # L2 regularization</code></pre>
<p>The Activation Functions used are: Rectifier, Tanh, Maxout, and we use different Drop-out Ration. The <strong>Drop-out</strong> process is about <a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning"><strong>Pruning</strong></a> some connections between neurons during training. It is very attractive to know that the <a href="https://en.wikipedia.org/wiki/Synaptic_pruning"><strong>human brain do the same pruning process in the early stage of the birth</strong></a>.</p>
<p>Now, we need to define the search criteria, and in the following example we use the <strong>Random Search</strong> criteria.</p>
<pre class="r"><code># Set the search criteria
search_criteria &lt;- list(
  strategy = &quot;RandomDiscrete&quot;, max_runtime_secs = 360, # maximum time in seconds for the entire Random or Grid
  max_models = 100, seed=1234567, stopping_rounds = 5, # maximum time in seconds for he model
  stopping_tolerance = 1e-2 # the model will stop if the ratio between the best model and reference is more or equal to 1e-2
)</code></pre>
<p>Now, we can use the <strong>h2o.grid()</strong> function to compute all the models defined. We specify <strong>deeplearning</strong> as the algorithm. We also defined to stop the training every single model when the <strong>logloss</strong> is not improved more than 1% after 2 different evaluations. </br> In the end, we can guess the best model using the function <strong>h2o.getModel()</strong> and taking the first model, since the models are ordered.</p>
<p>About the <a href="http://wiki.fast.ai/index.php/Log_Loss"><strong>Log-Loss</strong></a> function a simple explanation is the following: </br> consider to have the Predicted Probability Distribution of a Supervised Classifier, and also the True Distributon. We can use the Cross Entropy to estimate the difference between the Predicted Distribution and the True Distribution: this is called the Cross Entropy Loss or the Log-Loss. </br> If the predicted probability calculated from the Log-Loss is closed to 1, then the cost will be closed to the True Distribution.</p>
<p>About <a href="https://www.quora.com/What-is-epochs-in-machine-learning"><strong>Epochs</strong></a> is deep learning an epoch is a hyperparameter which is defined before training a model. One epoch is when an entire dataset is passed both forward and backward through the neural network only once. </br> Using all your batches once is 1 epoch. If you have 10 epochs it mean that you’re gonna use all your data 10 times (split in batches).</p>
<pre class="r"><code># Define Grid
# dl_random_grid &lt;- h2o.grid(
#   algorithm = &quot;deeplearning&quot;, # use artificial neural network with unlimited number of hidden layers
#   grid_id = &quot;dl_grid_random&quot;, # use Random Search
#   training_frame = train_h2o,
#   validation_frame = valid_h2o,
#   x = 1:9, # predictors
#   y =  10, # dependent variable
#   epochs = 1, # all bathces once
#   # stop when logloss does not improve by more than 1% after 2 evaluation
#   stopping_metric = &quot;logloss&quot;, # cross entropy
#   stopping_tolerance = 1e-2,
#   stopping_rounds = 2,
#   hyper_params = hyper_params,
#   search_criteria = search_criteria # random search
# )
# 
# 
# # Recover the Grid
# grid &lt;- h2o.getGrid(&quot;dl_grid_random&quot;, sort_by=&quot;logloss&quot;,
#                     decreasing = FALSE)
# 
# grid@summary_table[1,]
# 
# # Take the best model with lowest logloss
# best_model &lt;- h2o.getModel(grid@model_ids[[1]])
# best_model</code></pre>
<p>Using Grid and Random Search we can find out our model automatically maximazing the accuracy on the validation set.</p>
