---
title: The Activation Function
author: andrea perlato
date: '2019-06-20'
slug: the-activation-function
categories:
  - artificial intelligence
tags:
  - activation function
---

<style>
body {
text-align: justify}
</style>

What an **artifical neuron** do is to calculate a **weighted sum** of its input, adds a bias and then decides whether it should be “fired” or not.

<center>
![](/img/artificialneuron.png){width=30%}
</center>

considering the neuron of the figure above, the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. How do we decide whether the neuron should fire or not? We use **activatioin functions** for this purpose. To check the Y value produced by a neuron and decide whether outside connections should consider this neuron as fired or not.

<center>
![](/img/activatioinfunction.png){width=60%}
</center>

Generally, we have four different types of activation function that we can choose from. Of course, there are more different types, but these four are the predominate ones. </br>
The [**Threshold Function**](https://en.wikipedia.org/wiki/Activation_function) the the simplest function where if the value is less than zero then the function is zero, but if the value is more thae zero the function is one. It is basically a yes/no type ofo function.

<center>
![](/img/thresholdfunction.png){width=40%}
</center>

The [**Sigmoid Function**](https://en.wikipedia.org/wiki/Activation_function) the value **x** in its  formula is the value of the sum of the weighed. It is a function which is used in the **logistic regression**. It has a gradual progression and anything below zero is just zero, and above zero it approximates to one. It is expecially useful in the output layer, especially when we are tring to predict probabilities. The logistic sigmoid function can cause a neural network to get stuck at the training time. The [**Softmax Function**](https://en.wikipedia.org/wiki/Softmax_function) is a more generalized logistic activation function which is used for multiclass classification.
The sigmoid function is used mostly used in classification type problem since we need to scale the data in some given specific range with a threshold.

<center>
![](/img/sigmoidfunction.png){width=40%}

</center>

The [**Rectifier Function**](https://en.wikipedia.org/wiki/Activation_function) is one of the most popular functions for artificial neural networks so when its goes all the way to zero it is zero, and from there it is fradually progresses as the input value increase as well. It is used in almost all the **convolutional neural networks** or deep learning. 

<center>
![](/img/rectifierfunction.png){width=40%}
</center>

The [**Hyperbolic Tangent tanh Function**](https://en.wikipedia.org/wiki/Activation_function) is very similar to the sigmoid function, but here the hyperbolic tangent function goes below zero, and that could be useful in some application. The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. The tanh function is mainly used in classification between two classes.

<center>
![](/img/hypebolicfunctio.png){width=40%}
</center>

Now, considering for example to have an  0/1 outpu, we can use as activation function the threshold function or the sigmoid function. The advantage of the sigmoid function is that we have the probability of the output to be yes or no.

<center>
![](/img/exampleactivationfunction.png){width=60%}
</center>

The sigmoid activation function tells us the probability of the output to be equal to one. </br>
What is commonly used is to apply the **rectifier activation function** for the **hidden layer** and then the signals are passed on to the **output layer** where the **sigmoid activation function** is used, and that will be the final output that predict the probability.










































































































































