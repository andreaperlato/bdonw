---
title: Classification and Prediction with Support Vector Machine
author: andrea perlato
date: '2019-04-03'
slug: classification-and-prediction-with-support-vector-machine
categories:
  - machine learning
tags:
  - support vector machine
---



<style>
body {
text-align: justify}
</style>
<p>Support Vector Machine SVM is a linear classifier. We can consider SVM for linearly separable binary sets. The goal is to design a hyperplane (is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes). </br> The hyperplane classifies all the training vectors in two classes. We can have many possible hyperplanes that are able to classify correctly all the elements in the feature set, but the best choice will be the hyperplane that leaves the Maximum Margin from both classes. With Margins we mean the distance between the hyperplane and the closest elements from the hyperplane.</p>
<pre class="r"><code>data(iris)
summary(iris)</code></pre>
<pre><code>  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300  
 Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
       Species  
 setosa    :50  
 versicolor:50  
 virginica :50  
                
                
                </code></pre>
<pre class="r"><code>head(iris)</code></pre>
<pre><code>  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa</code></pre>
<pre class="r"><code># library(ggplot2)
# qplot(Petal.Length, Petal.Width, data=iris, color = Species)</code></pre>
<p>We are using the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set"><strong>iris</strong></a> dataset with 4 numerical variables and 1 factor which has 3 levels as described above. We can also see that the numerical variables have different ranges, it is a good pratice to normalize the data. </br> We create classification machine learning model that help us to predict the correct species. From the graph above, we can see there is a separation based on the <strong>Species</strong>, for example <strong>setosa</strong> species is very far from the other two groups, and between <strong>versicolor</strong> and <strong>virginica</strong> there is a small overlap. </br></p>
<p>With <strong>Support Vector Machine SVM</strong> we are looking for optimal separating hyperplane between two classes. And to do that SMV maximize the margin around the hyperplane. The point that lie on the boundary ar called <strong>Support Vectors</strong>, and the middle line is the <strong>Seprarating Hyperplane</strong>. </br> In situatins where we are not able to obtain a linear separator, the data are projected into a higher dimentional space, so that, data points, can become linearly separable. </br> In this case, we use the the <strong>Kernel Trick</strong>, using the <strong>Gaussian Radial Basis Function</strong>.</p>
<pre class="r"><code>library(e1071)
mymodel &lt;- svm(Species~., data=iris)
summary(mymodel)</code></pre>
<pre><code>
Call:
svm(formula = Species ~ ., data = iris)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.25 

Number of Support Vectors:  51

 ( 8 22 21 )


Number of Classes:  3 

Levels: 
 setosa versicolor virginica</code></pre>
<pre class="r"><code># Plot two-dimensional projection of the data with highlighting classes and support vectors
# The Species classes are shown in different shadings
plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4)) # specify a list of named values for the dimensions held constant</code></pre>
<p><img src="/MLPost/2019-04-03-classification-and-prediction-with-support-vector-machine_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Confusion Matrix and Missclassification Error
pred &lt;- predict(mymodel, iris) 
tab &lt;- table(Predicted = pred, Actual = iris$Species)
tab</code></pre>
<pre><code>            Actual
Predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         48         2
  virginica       0          2        48</code></pre>
<pre class="r"><code># Missclassification Rate
1-sum(diag(tab))/sum(tab)</code></pre>
<pre><code>[1] 0.02666667</code></pre>
<p>As we can see from the result above, we use Gaussian Radial Basis Function, <strong>cost</strong> is the constaint violation. </br> The two-dimensional plot above, is a projection of the data with highlighting classes and support vectors. The <strong>Species</strong> classes are shown in different shadings. Inside the <strong>blue class setosa</strong> we have 8 points depicted with a cross, and these are the suppor vectors for <strong>setosa</strong>. Similarly, we have points depicted with red cross points for <strong>versicolor</strong>, and green cross points for <strong>virginica</strong>. </br></p>
<p>From the <strong>Confusion Matrix</strong> above, we have only 2 observation missclassified for <strong>versicolor</strong>, and 2 observation missclassified for <strong>virginica</strong>. </br> We have also a missclassification rate, of <strong>2.6%</strong>. </br> If we try to use SVM with a <strong>linear kernel</strong> (not shown here), instead of a SVM with a <strong>radial kernel</strong>, the missclassification rate is a bit higher.</p>
<pre class="r"><code>mymodel &lt;- svm(Species~., data=iris,
               kernel = &quot;polynomial&quot;)

plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4))</code></pre>
<p><img src="/MLPost/2019-04-03-classification-and-prediction-with-support-vector-machine_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>pred &lt;- predict(mymodel, iris) 
tab &lt;- table(Predicted = pred, Actual = iris$Species)

1-sum(diag(tab))/sum(tab)</code></pre>
<pre><code>[1] 0.04666667</code></pre>
<p>If we also try to use a SVM with a <strong>polynomial kernel</strong>, as we can see from the graph above, the missclassification rate is increased to <strong>4.6%</strong>.</p>
