---
title: Polynomial Regression & Smoothing Splines
author: andrea peralto
date: '2019-03-27'
slug: polynomial-regression-smoothing-splines
categories:
  - machine learning
tags:
  - polynomial regression
  - smoothing splines
---

<style>
body {
text-align: justify}
</style>


**Polynomial Linear Regression**
Polynomial Linear Regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y,  and has been used to describe nonlinear phenomena such as the progression of disease epidemics. Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function is linear in the unknown parameters that are estimated from the data. It is a linear conbination of coefficients that are unknowns. For this reason, polynomial regression is considered to be a special case of multiple linear regression.  </br>
How to find the best fitting live from the graph above is the linear model method.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
library(ggplot2)
dat <- read.csv("C:/07 - R Website/dataset/ML/Dat.csv", sep = ";")

# Linear regression
lm(y ~ x, data=dat)

```

Having the coefficinet of the linear model, calculated above, we can plot the trend line.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
f <- function(x){
     return(34.04*x-65.27)
}

ggplot() + 
  geom_point(data=dat, aes(x=x, y=y)) + 
  stat_function(data=data.frame(x=c(-5,15)), aes(x=x), fun=f) # define left and right extremes of the dataframe and plot the function f

```

Now that we have the linear trend, we can go along the least squares line, and for each we get the point that correspond to it on the trend line.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
x <- dat$x
y <- f(x) # f is the function defined above based on the fitting line
means <- data.frame(x,y) # create a data frame with values of x and the fitted values of y

dat$group <- 1:100 # create a vectro from 1 to 100
means$group <- 1:100 # create a vectro from 1 to 100
groups <- rbind(dat, means)



# Add the fitted point to the graph
ggplot() + 
  geom_point(data=dat, aes(x=x, y=y)) + 
  stat_function(data=data.frame(x=c(-5,15)), aes(x=x), fun=f) + # define left and right extremes of the dataframe and plot the function f
  geom_point(data=means, aes(x=x, y=y), color='red', size=2) + # add the fitted data points
  geom_line(data=groups, aes(x=x, y=y, group=group))

# Calculate the Sum of the Square Residual
sum((dat$y-means$y)^2)

```

The distances that we plotted above are the residuals, and if we want to understand how well the trend line fits the data, we have to square ech of the black lines (residuals) shown in the graph above, then sum up all of them and this process is called Resuduals Sum of Square. From this calculation we found **158423.5** which is a very high value. We can try to fit another kind of polynomium, but not a line in order to get a better fit. </br>
So, now we try to fit a quadratic polynomium and a polynomial of degree three.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
# Second degree polynomium
lm(y ~ x + I(x^2), data=dat)

# Define a function for the polynomial
f <- function(x){
     return(2.9522*x^2+0.9719*x-0.5685)
}

means$y <- f(means$x) # calculate fitted points
groups <- rbind(dat, means) 

# Add the fitted point to the graph
ggplot() + 
  geom_point(data=dat, aes(x=x, y=y)) + 
  stat_function(data=data.frame(x=c(-5,15)), aes(x=x), fun=f) + # define left and right extremes of the dataframe and plot the function f
  geom_point(data=means, aes(x=x, y=y), color='red', size=2) + # add the fitted data points
  geom_line(data=groups, aes(x=x, y=y, group=group))

# Calculate the Sum of the Square Residual
sum((dat$y-means$y)^2)

```

as we can see above, now we have a mich smaller Sum of the Square Residuals equal to **34582.44**, and this is a much better fit.
Now we can try to add one more degree and create a polynomial of degree three.











The main difference between polynomial and spline is that polynomial regression gives a single polynomial that models your entire data set. 
Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.




