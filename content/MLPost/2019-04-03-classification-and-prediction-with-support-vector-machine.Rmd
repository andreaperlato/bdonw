---
title: Classification and Prediction with Support Vector Machine
author: andrea perlato
date: '2019-04-03'
slug: classification-and-prediction-with-support-vector-machine
categories:
  - machine learning
tags:
  - support vector machine
---

<style>
body {
text-align: justify}
</style>

Support Vector Machine SVM is a linear classifier. We can consider SVM for linearly separable binary sets. The goal is to design a hyperplane (is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes). </br>
The hyperplane classifies all the training vectors in two classes. We can have many possible hyperplanes that are able to classify correctly all the elements in the feature set, but the best choice will be the hyperplane that leaves the Maximum Margin from both classes. With Margins we mean the distance between the hyperplane and the closest elements from the hyperplane.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
data(iris)
summary(iris)
head(iris)

# library(ggplot2)
# qplot(Petal.Length, Petal.Width, data=iris, color = Species)

```

We are using the [**iris**](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset with 4 numerical variables and 1 factor which has 3 levels as described above. We can also see that the numerical variables have different ranges, it is a good pratice to normalize the data. </br>
We create classification machine learning model that help us to predict the correct species.
From the graph above, we can see there is a separation based on the **Species**, for example **setosa** species is very far from the other two groups, and between **versicolor** and **virginica** there is a small overlap. </br>

With **Support Vector Machine SVM** we are looking for optimal separating hyperplane between two classes. And to do that SMV maximize the margin around the hyperplane. The point that lie on the boundary ar called **Support Vectors**, and the middle line is the **Seprarating Hyperplane**. </br>
In situatins where we are not able to obtain a linear separator, the data are projected into a higher dimentional space, so that, data points, can become linearly separable. </br>
In this case, we use the the **Kernel Trick**, using the **Gaussian Radial Basis Function**.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
library(e1071)
mymodel <- svm(Species~., data=iris)
summary(mymodel)

# Plot two-dimensional projection of the data with highlighting classes and support vectors
# The Species classes are shown in different shadings
plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4)) # specify a list of named values for the dimensions held constant

# Confusion Matrix and Missclassification Error
pred <- predict(mymodel, iris) 
tab <- table(Predicted = pred, Actual = iris$Species)
tab

# Missclassification Rate
1-sum(diag(tab))/sum(tab)

```

As we can see from the result above, we use Gaussian Radial Basis Function, **cost** is the constaint violation. </br>
The two-dimensional plot above, is a projection of the data with highlighting classes and support vectors. The **Species** classes are shown in different shadings.
Inside the **blue class setosa** we have 8 points depicted with a cross, and these are the suppor vectors for **setosa**. Similarly, we have points depicted with red cross points for **versicolor**, and green cross points for **virginica**. </br>

From the **Confusion Matrix** above, we have only 2 observation missclassified for **versicolor**, and 2 observation missclassified for **virginica**. </br>
We have also a missclassification rate, of **2.6%**. </br>
If we try to use SVM with a **linear kernel** (not shown here), instead of a SVM with a **radial kernel**, the missclassification rate is a bit higher.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
mymodel <- svm(Species~., data=iris,
               kernel = "polynomial")

plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4))

pred <- predict(mymodel, iris) 
tab <- table(Predicted = pred, Actual = iris$Species)

1-sum(diag(tab))/sum(tab)

```

If we also try to use a SVM with a **polynomial kernel**, as we can see from the graph above, the missclassification rate is increased to **4.6%**. </br>

We can try to tune the model in order to have better classification rate. Tune is also called hyperparameter optimization, and it helps to select the best model.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
# Tuning
set.seed(123)

tmodel <- tune(svm, Species~., data=iris,
               ranges = list(epsilon = seq(0,1,0.1), # sequence from 0 to 1 with an icrement of 0.1 
              cost = 2^(2:7)))                       # cost captures the cost of constant violatio
                                                     # if cost is too high, we have penalty for non-separable points, and the model store too many support vectors

plot(tmodel)

```

We use **epsilon** and **cost** as tune paramentrs. </br>
The **cost** parameter captures the cost of constant violatio. If **cost is too high**, we have penalty for non-separable points, and as a consequence we have a model that store too many support vectors, leading to **overfitting**. On the contrary, if **cost is too small**, we may end up with **underfitting**. </br>

The value of **epsilon** defines a margin of tolerance where no penalty is given to errors. In fact, in SVM we can have **hard** or **soft** margins, where soft allow observations inside the margins. Soft margin is used when two classes are not linearly separable. </br>

the plot here above gives us the performance evaluation of SMV for the **epsilon** and **cost** parameters. Darker regions means better results, and so lower misclassification error. By interpreting this graph we can choose the best model parameters.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
mymodel <- tmodel$best.model
summary(mymodel)

plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4))

pred <- predict(mymodel, iris) 
tab <- table(Predicted = pred, Actual = iris$Species)

1-sum(diag(tab))/sum(tab)

```

Fomr the summary above, now we have **35 support vectors**: **6** for **setosa**, **15** for **versicolor**, and **14** for **virginica**.
The graph here above expain the result obtained with the best model. Looking at the confusion matrix and missclassification error, we can see that only 2 observations are missclassified and the missclassification error is 1.3% which is significant less from what the got earlier.















