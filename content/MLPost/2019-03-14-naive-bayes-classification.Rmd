---
title: Naive Bayes Classification
author: andrea perlato
date: '2019-03-14'
slug: naive-bayes-classification
categories:
  - machine learning
tags:
  - naive bayes
---

<style>
body {
text-align: justify}
</style>

Naive Bayes is an effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the [**Maximum A Posteriori decision rule**](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.

An intuitive explanation for the **Maximum A Posteriori Probability MAP** is to think probabilities as degrees of **belief**. For example, how likely are we vote for a candidate depends on our **prior belief**. We can modify our stand based on the **evidence**. Our final decision, based on evidence, is the **posterior belief**, which is what happens after we sifted through the evidence. </br>
**MPA** is simply the maximum posterior belief: after going through all the debates, what is your most likely decision. </br>

We use Naive Bayes in an example to predict, based on some features (e.g. the rank of the school student come from), if a student is admitted or rejected. 

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '80%'}
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
data <- read.csv("C:/07 - R Website/dataset/ML/binary.csv")
data$rank <- as.factor(data$rank)
data$admit <- as.factor(data$admit)

# Visualization
pairs.panels(data[-1])

data %>%
  ggplot(aes(x=admit, y=gre, fill=admit)) + 
  geom_boxplot()

data %>%
  ggplot(aes(x=gre, fill=admit)) + 
  geom_density(alpha=0.8, color='black')

```
In order to develop a model, we have to make sure that the independent variables are not highly correlated. </br>
We can see from the scatterplot above that the only numerical variables are **gre** and **gpa**, and they are not strogly correlated (R-squared=0.38).
Moreover, looking at the boxplot that compare **admit** as a function of **gre**, there is a significant overlap between the two levels of admit.
From the dnsity plot of **gre** as a function on **admit**, we can see that students not admitted (admit=1) have higher **gre** comapared to student admitr√¨ted (admit=0). Anyway, there is a significat amount of overlap between the two distributioins. </br>
The same is for **gpa**, here not shown. </br>


```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '80%'}
# Data Preparation
set.seed(1234)
ind <- sample(2, nrow(data), replace=TRUE, prob = c(0.8, 0.2))
train <-data[ind == 1,]
test <-data[ind == 2,]

```

Now, the probability of a student admitted given he belongs to rank one: p(admit=1|rank=1) is equal to: 
**p(admit=1)*p(rank=1|admit=1)/p(rank=1)**

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '80%'}
# Naive Bayes Model
model <- naive_bayes(admit ~ ., data = train)
model
plot(model)

```

From the result above, we have **A Priori** probabilities to be admitted of 0.31: only 31% of the students were admitted to the program.
Moreover, for numerical variables (gre, gpa) we have mean and standard deviation, and for categorical variables (rank) we have probabilities. For example, p(rank=1|admit=0)=0.103 and p(rank=1|admit=1)=0.245, as we can see from the rsult table above. </br>
We have also three plots of the model that represent the density of the numerical variables and the bars for the categorical variable. </br>


























