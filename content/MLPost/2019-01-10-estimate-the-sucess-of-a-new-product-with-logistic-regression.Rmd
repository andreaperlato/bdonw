---
title: Assessing the sucess of a new product via Logistic Regression
author: andrea perlato
date: '2019-01-10'
slug: estimate-the-sucess-of-a-new-product-with-logistic-regression
categories:
  - machine learning
tags:
  - logistic regression
---

<style>
body {
text-align: justify}
</style>

These are a series of analysis to illustate the main classification algorithms and their advantages.
The table shows the business clients of a company that has just launched a new product online. Some of the clients responded positively to the ads by buying the product and other responded negatively by not buying the product. The last column of the table tells for each user if the user bought the product or not.


```{r, echo=FALSE, comment=NA, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(formattable)
dataset <- read.csv("C:/07 - R Website/dataset/ML/Social_Network_Ads.csv")
dt <- dataset[1:8, 1:5]
kable(dt) %>%
  kable_styling(bootstrap_options = "responsive", full_width = T, position = "center")

dataset = dataset[3:5]

```

Our mission is to use the main classifier algorithms and compare the best result. In this specific post we used Logistic Regression.

**Splitting the dataset into the Training set and Test set.** 
```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE) 

```

**Feature Scaling.** <br/>
For classification is better to do feature scaling in order to have an accurate prediction.
Since most of the machine learning algorithms use Euclidian distance between two data point in their computations, this is a problem. <br/>
To supress this effect, we need to bring all feature to the same level of magnitudes. This can be achived by scaling.
```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])

```

**Fitting Logistic Regression to the Training set.**
```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
classifier = glm(formula = Purchased ~ .,
                 family = binomial,
                 data = training_set)

```

**Predicting the Test set results and Confusion Matrix at a cut-off of 0.5**
```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)

prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)

cm = table(test_set[, 3], y_pred > 0.5)
cm

```

**Visualising the Test set results.**
```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE}
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
     main = 'Logistic Regression (Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

```
