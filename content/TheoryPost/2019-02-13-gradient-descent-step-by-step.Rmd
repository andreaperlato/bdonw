---
title: Gradient Descent Step by Step
author: andrea perlato
date: '2019-02-13'
slug: gradient-descent-step-by-step
categories:
  - theory
tags:
  - gradient descent
---

<style>
body {
text-align: justify}
</style>


**INTRODUCTION** </br>
In statistics, Machine Learning and other Data Science fields, we optimize a lot of stuff. For example in linear regresion, we optimize the Intercept and Slope, or when we use Logistic Regression we optimize the squiggle. Moreover, in t-SNE we optimize clusters. The interesting thing is that gradient descent can optimize all these things and much more.
A good example is the **Sum of the Squared Residuals** in Regression: in Machine Learning lingo this is a type of **Loss Function**.
The **Residual** is the difference between the Observed Value and the Predicted Value.

<center>
![](/img/gradientregression.png){width=100%}
</center>

The figure above shows on the y-axis the sum of the squared residuals and the x-axis different value for the intercept.
The firt point on the y-axis represent the sum of the squared residuals when the intercept is equal to zero. We continue to plot point on the graph based on different value of the intercept. The lowest poin in the graph has the lowest sum of the squared residuals.
Gradient descent identifies the optimal value by taking big steps when we are far away to the optimal sum of the squared residual, and start to make many steps when it is close to the best solution.

