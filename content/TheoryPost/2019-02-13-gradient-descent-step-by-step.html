---
title: Gradient Descent Step by Step
author: andrea perlato
date: '2019-02-13'
slug: gradient-descent-step-by-step
categories:
  - theory
tags:
  - gradient descent
---



<style>
body {
text-align: justify}
</style>
<p><strong>INTRODUCTION</strong> </br> In statistics, Machine Learning and other Data Science fields, we optimize a lot of stuff. For example in linear regresion, we optimize the Intercept and Slope, or when we use Logistic Regression we optimize the squiggle. Moreover, in t-SNE we optimize clusters. The interesting thing is that gradient descent can optimize all these things and much more. A good example is the <strong>Sum of the Squared Residuals</strong> in Regression: in Machine Learning lingo this is a type of <strong>Loss Function</strong>. The <strong>Residual</strong> is the difference between the Observed Value and the Predicted Value.</p>
<center>
<img src="/img/gradientregression.png" style="width:100.0%" />
</center>
<p>The figure above shows on the y-axis the sum of the squared residuals and the x-axis different value for the intercept. The firt point on the y-axis represent the sum of the squared residuals when the intercept is equal to zero. We continue to plot point on the graph based on different value of the intercept. The lowest poin in the graph has the lowest sum of the squared residuals. Gradient descent identifies the optimal value by taking big steps when we are far away to the optimal sum of the squared residual, and start to make many steps when it is close to the best solution. Than we can calculate the <strong>derivate d</strong> of each point of the function created by the points. In other words, we are taking the derivative of the <strong>Loss Function</strong>.</p>
<center>
<img src="/img/gradientderivate.png" style="width:60.0%" />
</center>
<p>Gradient Descent uses derivative in order to find where the Sum of the Squared Residuals is lowest. The closer we get to the optimal value for the intercept, the closer the slope of the curve gets to zero.</p>
<p><strong>HOW DOES GRADIENT DESCENT KNOW TO STOP TAKING STEPS?</strong> </br> Gradient Descent stops when the step size is very close to zero, and the step size is very close to zero qhen the <strong>slop size</strong> is close to zero.</p>
<center>
<img src="/img/gradientstepsize.png" style="width:60.0%" />
</center>
<p>In practice, the <strong>Minimum Step Size</strong> is equal to <strong>0.001</strong> or smaller. Moreover, Gradient Descent includes a limit on the number of steps it will take before giving up. In practice, the <strong>Maximum Number of Steps</strong> is equal to <strong>1000</strong> or greater.</p>
<p>We can also estimate the intercept and the slope simultaneously. We use the Sum of the Squared Residuals as the Loss Function, and we can represent a <strong>3D graph of the Loss Function</strong> for different values of intercept and the slope. </br> We want to find the values for the intercept and slope that give us the minumum Sum of the Squared Residuals.</p>
<center>
<img src="/img/gradientinterceptslope.png" style="width:60.0%" />
</center>
<p>So, just like before, we need to take the derivative of the function represented by the graph above for both intercept and slope. </br> When we have two or more derivatives of the same function (in this case the derivative or both intercept and slope) we call this a <strong>GRADIENT</strong>. </br> We will use this <strong>GRADIANT</strong> to <strong>DESCENT</strong> to lowest point in the Loss Function, which, in this case, is the Sum of the Squared Residuals. </br></p>
<p>There are tons of other Loss Functions than the Sum of the Squared Residuals, and these Loss Functions work with other types of data. Regardless of which Loss Function is used, Gradient Descent works the same way. </br></p>
<p>Summarizing: </br> 1 - Take the derivative of the Loss Function for each parameter in it. In ML lingo, take the Gradient of the Loss Function. </br> 2 - Pick random values for the parameters. </br> 3 - Plug the paramenter values into the derivatives (Gradient). </br> 4 - Calculate the Step Sizes: Step Size = slope * Learning Rate. </br> 5 - Calculate the New Parameters: NewParameter = Old Parameter - Step Size. </br> 6 - Go back to step 3 and repeat untill Step Size is very small, or when the Maximum Number of Steps is reached. </br></p>
<p>One last thing is that when we have millions of data points, the process take a long time. </br> There is a thing called <strong>Stochastic Gradient Descent</strong> that uses a randomly selected subset of the data at every step rather than the full data. </br> This reduces the time spent calculating the derivatives of the Loss Function.</p>
