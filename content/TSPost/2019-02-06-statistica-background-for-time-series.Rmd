---
title: Statistical Background for Time Series
author: andrea perlato
date: '2019-02-06'
slug: statistica-background-for-time-series
categories:
  - time series
tags:
  - time series analysis
  - forecasting
---

<style>
body {
text-align: justify}
</style>

In this post we will review the statistical background for time series analysis and forecasting. </br>
We start about how to compare different time seris models against each other.

**Forecast Accuracy** </br>
It determine how much difference thare is between the actual value and the forecast for the value. The simplest way to m ake a comparison is via scale dependent error because all the models need to be on the same scale using the Mean Absolute Error - MAE and the Root Mean Squared Error - RMSE.
[**MAE**](https://en.wikipedia.org/wiki/Mean_absolute_error) is the mean of all differences between actual and forecaset absolute value and in order to avoid negative values we can use [**RMSE**](https://en.wikipedia.org/wiki/Root-mean-square_deviation). We have also the [**Mean Absolute Scaled Error - MASE**](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error) that measure the forecast error compared to the error of a naive forecast. </br>
When he have a MASE = 1, that means the model is exactly as good as just picking the last observation. </br>
An MASE = 0.5, means that our model has doubled the prediction accuracy. The lower, the better. </br> 
When MASE > 1, that means the model needs a lot of improvement. </br> 

The [**Mean Absolute Percentage Error - MAPE**](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error), measures the difference of forecast errors and divides it by the actual observation value. The crucial point is that MAPE puts much more weight on extreme values and positive errors, which makes MASE a favor metrics. But the big benefit of MAPE is the fact that it is scale independent: that means we can use MAPe to compare a model on different datasets.

Quire often, [**Akaike Information Criterion - AIC**](https://en.wikipedia.org/wiki/Akaike_information_criterion)  is used. </br> 
It is extensively used also in statistical modeling and machine learning to compare different models. The key point of AIC is that it penalizes more complex models.

We start as an example with a random time series.
We divide the series into a training and a test set using the window() function. With window() function we can easily extract a time frame, in this case we take the part of the data starting from 1818 and ending in 1988. It is a beautiful fuction to split time series.
The training data is used to fit the model and the test set is used to see how well the model performs.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
set.seed(95)
myts <- ts(rnorm(200), start = (1818))
mytstrain <- window(myts, start = 1818, end = 1988) # training set
plot(mytstrain)
           
library(forecast)
meanm <- meanf(mytstrain, h=30) # mean method and we get 30 observation in to the future
naivem <- naive(mytstrain, h=30) # naive method
drifm <- rwf(mytstrain, h=30, drift = TRUE) # drift method

mytstest <- window(myts, start = 1988)
accuracy(meanm, mytstest)
accuracy(naivem, mytstest)
accuracy(drifm, mytstest)

```

In the results we can see the error of both training and test set. It is also calculated the difference between the actual values from mytstest and the forecasted values from the model. </br> 
As expected in all four of our statistics the mean method is the best, with the lowest value for RMSE, MAE, MAPE and MASE. That is as expected because we are working with a random data with zero mean and without any drift season.


**The importance of Residuals in Time Series Analysis** </br>
Residuals tell a lot about the quality of the model. </br>
The rule of thumb is: only randomness should stay in the residuals. They are the container of randomness (data that we cannot explain in mathematical terms) and ideally the have a mean of zero and constant variance. </br>
Residuals should be uncorrelated, correlated residuals still have information left in them, and ideally they are normally distributed.
That is aso caed random walk (each vaue is a random step away from the preavious value).

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
# get-rid of the NA in the drift method
driftwithoutNA <- drifm$residuals
driftwithoutNA <- driftwithoutNA[2:165]

hist(drifm$residuals) # test residuals
acf(driftwithoutNA) # test auto-correlation

```

As we can see from the graph above, we have a peak at around zero and the tails to both ends look quite equal which is what would expect with the normal distribution of the residuals. From the acf() function (autocorreation function), we can test for auto-correlation: if we have several of the vertical bars above the threshold level, we have auto-correlation. </br>
Here, we can see that 4/30 bars are over/below the threshold, and that means residuals have information left in them. This means the model can be improved upon.


**Autocorrelation** </br>
It is a statistical term which describes the orrelation (or the ack of such) in a time series dataset. It is a key statistic because it tels us whether preovious observations infuence the recent one. It is a correlation on a time scale. If we have a random wak there are not any autocorretion.
To test it we can again use the acf() function (autocorreation function) or the partia autocorreation function pacf().
**Autocorreation** in time series is simpy the correlation coefficient between different time points (lags) in a time series.
** Partial Autocorrelation** is the correlation coefficient abjusted for all shorter lags in a time series. </br>

The acf() is used to identify the moving average part of the ARIMA mode, whie pacf() identifies the vaues for the autoregresiove part.


```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
acf(lynx, lag.max = 20); pacf(lynx, lag.max = 20)

```

In the example above, there are sevelar bas ranging out of the 95% confidence intervals (we omit first bar because it is the autocorrelation against isef at lag0).
On the contrary, pacf() starts at lag1.

**Determining the Forecasting Method** </br>
Quantitative forecasting can be divided in Linear: </br>
Exponential Smoothing: better in seasonaity </br>
ARIMA: better in autoregression </br>
Sesonal Decomposition: dataset needs to be seasonal (minimum number of seasonal cyces of 2) </br> 
Non-Linear models: </br> 
Neural Net: input is compressed to several neurons </br>
SVM: very limited in time series </br>
Clustering: aclustering approach to time series </br>

**Univariate Seasonal Time Series** </br>
The main modeling oprions are: Seasonal ARIMA, Holt-Winters Exponential Smooting and Seasonal Decomposition.
With **Seasonal Decompositioin** we divide components in trend, seasonaity and random data.  This approach has several drawbacks that are: </br>
1 - slow to catch sudden changes </br>
2 - the model assumes that the seasonal component stays constan: this is probematic when we have ong time series where the seasona changes </br>

It the foowing example we wi use the [**AirPassengers**](https://www.kaggle.com/rakannimer/air-passengers) dataset to expalin the Seasonal Decomposition.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
plot(AirPassengers)

# additive mode
mymodel1 <- decompose(AirPassengers, type = "additive")
plot(mymodel1)

```

The graph shows a seasona pattern along with a trend. It is also quite evident that the seasonality increases quite a bit at the ast of the plot. </br>
The ast line of the Decomposition Graph is the random component that shows a pattern on the left and right. The middle part of the plot around 1954 and 1958 seems random, but the rest of the time series shows peaks at a certain interval. that is not a good sign and shows that the model coud be improved upon. 

**Exponential Smoothing with ETS** </br>
It describes the time series with three parameters: </br>
Error: addictive, mutiplicative </br>
Trend: non-present, addictive, mutiplicative </br>
Seasonaity: non-present, addictive, multipicative </br>

We can use **Additive Decomposition Method** that adds the Error, Trend and seasonality up. Or **Mutipicative Decomposition Method** that mutiplies these components. In general, if the seasona component stays constant over several cycles, it is best to use the Additive Decomposition Method. </br>
Parameters can also be mixed: additive trend with mutipicative seasonaity: that is the process of the **Mutipicative Hot-Winters Mode**. </br>
In **Exponential Smoothing** recent data is given more weight than older observations.
This method has smooting coefficients to manage the weighting based on the timestamp: </br>
1 - reactive modes relies heavly on recent data, thta results in high coefficients </br>
2 - smooth models have low coefficients </br>
The coefficients are: </br>
alpha: initial level </br>
beta: the trend </br>
gamma: seasonality </br>

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
library(forecast)
etsmodel <- ets(nottem); etsmodel

```

The first info we get from the results is the model type itself: ETS(A,N,A) that is about additive error, no trend and additive seasonality, that is quite what we woud expect from a temperature based data. </br>
We also see from the resuts alpha (error) and gamma (seasonaity) very closed to zero. </br>
We also see the Initial level called l = 48.92, sigma = 2.25 and three information criteria: </br>
AIC, AICc, BIC. </br> 

Let's see how the mode looks compared to the original dataset and the forecast.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
plot(forecast(etsmodel$fitted, h = 12)) # plot the forecast

```
The graph gives us an extra year in blue with prediction intervals (80% accuracy and 95% accuracy).
Let's try now mutiplicative method.

```{r, echo=TRUE, comment=NA, message=FALSE, warning=FALSE, fig.align='center', out.width = '100%'}
etsmodmut <- ets(nottem, model = "MZM"); etsmodmut

```

Now, with the mutiplicative method the information criteria (AIC, AICc, BIC) are higher and that means the model is not as good as the one we got from out initially.

<center>
![](/img/tsmodel.png){width=100%}

</center>

In fact, if we look at the graph comparison above, the mutiplicative method does not catch the peacks as well as the initia one. There is a big distance between the back peaks and the red peaks.

















