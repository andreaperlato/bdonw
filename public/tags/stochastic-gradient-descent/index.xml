<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stochastic Gradient Descent on Andrea Perlato</title>
    <link>/tags/stochastic-gradient-descent/</link>
    <description>Recent content in Stochastic Gradient Descent on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/stochastic-gradient-descent/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stochastic Gradient Descent</title>
      <link>/aipost/stochastic-gradient-descent/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/stochastic-gradient-descent/</guid>
      <description>body { text-align: justify}  In Gradient Descent, we used the sum of the squared residuals as the Loss Function to determine how well the initial line fit the data. Than, we took the derivative of the sum of the squared residuals with respect to the intercept and slope. We repeated that process a lot of times untill we took the maximum number of steps, or the spteps became very, very small.</description>
    </item>
    
  </channel>
</rss>