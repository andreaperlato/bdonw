<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Deal Multicollinearity with LASSO Regression - Andrea Perlato</title>
<meta property="og:title" content="Deal Multicollinearity with LASSO Regression - Andrea Perlato">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Deal Multicollinearity with LASSO Regression</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used). </br> To reduce multicollinearity we can use regularization that means to keep all the features but reducing the magnitude of the coefficients of the model. <strong>This is a good solution when each predictor contributes to predict the dependent variable</strong>. </br></p>
<p>LASSO Regression is similar to RIDGE REGRESSION except to a very important difference. The <strong>Penalty Function</strong> now is: lambda*|slope| </br> The result is very similar to the result given by the <strong>Ridge Regression</strong>. Both can be used in logistic regression, regression with discrete values and regression with interaction. </br> The big difference between <strong>Ridge</strong> and <strong>LASSO</strong> start to be clear when we <strong>increase</strong> the value on <strong>Lambda</strong>.</p>
<p>The advantage of this is clear when we have LOTS of PARAMETERS in the model: </br> In <strong>Ridge</strong>, when we increase the value of LAMBDA, the most important parameters might shrink a little bit and the <strong>less important parameter stay at high value</strong>. In contrast, with <strong>LASSO</strong> when we increase the value of LAMBDA the most important parameters shrink a little bit and the <strong>less important parameters goes closed to ZERO</strong>. </br> <strong>So, LASSO is able to exclude silly parameters from the model</strong>. </br></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    

    
  </body>
</html>

