<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Extreme Gradient Boosting Algorithm - Andrea Perlato</title>
<meta property="og:title" content="Extreme Gradient Boosting Algorithm - Andrea Perlato">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Extreme Gradient Boosting Algorithm</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>Extreme Gradient Boosting is extensively used because is fast and accurate, and can handle missing values. </br> Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. </br> XGBoost is one of the implementations of Gradient Boosting concept, but what makes XGBoost unique is that it uses a more regularized model formalization to control over-fitting, which gives it better performance. </br></p>
<p>We use it in an example to predict, based on some features (e.g. the rank of the school student come from), if a student is admited or rejected.</p>
<pre class="r"><code># Partition Data
set.seed(1234)
ind &lt;- sample(2, nrow(data), replace = T, prob = c(0.8, 0.2))
train &lt;- data[ind==1,]
test &lt;- data[ind==2,]

# Create matrix One-Hot Encoding for Factor variables.
# One-Hot Encoding convert our data in a numeric format, as required by XGBoost.
# It convert the variable rank in a sparse matrix, in order to have dummy variable for each rank.
trainm &lt;- sparse.model.matrix(admit ~ .-1, data = train) 
head(trainm)</code></pre>
<pre><code>6 x 6 sparse Matrix of class &quot;dgCMatrix&quot;
  gre  gpa rank1 rank2 rank3 rank4
1 380 3.61     .     .     1     .
2 660 3.67     .     .     1     .
3 800 4.00     1     .     .     .
4 640 3.19     .     .     .     1
6 760 3.00     .     1     .     .
7 560 2.98     1     .     .     .</code></pre>
<pre class="r"><code># Convert Train-Set in a Matrix
train_label &lt;- train[,&quot;admit&quot;] # define the responce variable
train_matrix &lt;- xgb.DMatrix(data = as.matrix(trainm), label = train_label)</code></pre>
<p>From the matrix above, we can see that One-Hot Encoding was made for the factor variabile (i.e. rank). We repeat the same procedure for the Test-set.</p>
<pre class="r"><code># Convert Test-Set in a Matrix
testm &lt;- sparse.model.matrix(admit ~ .-1, data = test) 
test_label &lt;- test[,&quot;admit&quot;] # define the responce variable
test_matrix &lt;- xgb.DMatrix(data = as.matrix(testm), label = test_label)</code></pre>
<p>For now, we have the matrix formatted in the proper format needed for the analysis. At this stage, we have to define the parameters of the model, and create it.</p>
<pre class="r"><code>nc &lt;- length(unique(train_label))
xgb_params &lt;- list(&quot;objective&quot; = &quot;multi:softprob&quot;,
                   &quot;eval_metric&quot; = &quot;mlogloss&quot;,
                   &quot;num_class&quot; = nc)

watchlist &lt;- list(train = train_matrix, test = test_matrix)

# Create the Extreme Gradient Boosting Model
bst_model &lt;- xgb.train(params = xgb_params,    # multiclass classification
                       data = train_matrix,
                       nrounds = 20,           # maximum number of interations
                       watchlist = watchlist)  # check what is going on</code></pre>
<pre><code>[1] train-mlogloss:0.594324 test-mlogloss:0.651085 
[2] train-mlogloss:0.534790 test-mlogloss:0.612848 
[3] train-mlogloss:0.483394 test-mlogloss:0.595096 
[4] train-mlogloss:0.454567 test-mlogloss:0.597930 
[5] train-mlogloss:0.423043 test-mlogloss:0.599238 
[6] train-mlogloss:0.385208 test-mlogloss:0.595708 
[7] train-mlogloss:0.372651 test-mlogloss:0.614298 
[8] train-mlogloss:0.355396 test-mlogloss:0.612562 
[9] train-mlogloss:0.345466 test-mlogloss:0.632218 
[10]    train-mlogloss:0.337584 test-mlogloss:0.649025 
[11]    train-mlogloss:0.321141 test-mlogloss:0.649074 
[12]    train-mlogloss:0.312773 test-mlogloss:0.664441 
[13]    train-mlogloss:0.309723 test-mlogloss:0.677517 
[14]    train-mlogloss:0.296634 test-mlogloss:0.677277 
[15]    train-mlogloss:0.284527 test-mlogloss:0.689391 
[16]    train-mlogloss:0.277117 test-mlogloss:0.684779 
[17]    train-mlogloss:0.270126 test-mlogloss:0.688089 
[18]    train-mlogloss:0.265546 test-mlogloss:0.701466 
[19]    train-mlogloss:0.260600 test-mlogloss:0.700825 
[20]    train-mlogloss:0.256453 test-mlogloss:0.717978 </code></pre>
<pre class="r"><code>bst_model</code></pre>
<pre><code>##### xgb.Booster
raw: 69.1 Kb 
call:
  xgb.train(params = xgb_params, data = train_matrix, nrounds = 20, 
    watchlist = watchlist)
params (as set within xgb.train):
  objective = &quot;multi:softprob&quot;, eval_metric = &quot;mlogloss&quot;, num_class = &quot;2&quot;, silent = &quot;1&quot;
xgb.attributes:
  niter
callbacks:
  cb.print.evaluation(period = print_every_n)
  cb.evaluation.log()
# of features: 6 
niter: 20
nfeatures : 6 
evaluation_log:
    iter train_mlogloss test_mlogloss
       1       0.594324      0.651085
       2       0.534790      0.612848
---                                  
      19       0.260600      0.700825
      20       0.256453      0.717978</code></pre>
<p>In the code above we specified to use the <strong>Softprob Function</strong>. In <a href="https://en.wikipedia.org/wiki/Softmax_function"><strong>Softmax</strong></a> we get the class with the maximum probability as output, and with Softprob we get a matrix with probability value of each class we are trying to predict. </br> We can also see that we have as output the total number of interations (in our example 20 interactions), and we can see what was the error in both train and test set (aka elaluation_log). </br> We have also some information about the model. The <strong>elaluation_log</strong> session of the output can be converted into a plot.</p>
<pre class="r"><code># Training and Test Error Plot
e &lt;- data.frame(bst_model$evaluation_log)
plot(e$iter, e$train_mlogloss, col = &#39;blue&#39;)
lines(e$iter, e$test_mlogloss, col = &#39;red&#39;)</code></pre>
<p><img src="/MLPost/2019-03-13-extreme-gradient-boosting-algorithm_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>min(e$test_mlogloss)</code></pre>
<pre><code>[1] 0.595096</code></pre>
<pre class="r"><code>e[e$test_mlogloss == 0.595096,]</code></pre>
<pre><code>  iter train_mlogloss test_mlogloss
3    3       0.483394      0.595096</code></pre>
<p>We can see from the graph above that the error, in the Training-set, is quite high in the beginning and as the interactions increase the error comes down. The little curve in red that we have on the top right of the graph is the error rate of the Test-set: initially the error quickly comes down but immediately increases. We can see that we have a Minimum Error of the Test-set of 0.59 at we reach it after 3 interations. </br> This red curve says that we have a significat overfitting. We need to find a better model.</p>
<pre class="r"><code># More feature to find the best model
bst_model &lt;- xgb.train(params = xgb_params,    # multiclass classification
                       data = train_matrix,
                       nrounds = 20,           # maximum number of interations
                       watchlist = watchlist,
                       eta = 0.05)              # is eta is low, the model is more robust to overfitting</code></pre>
<pre><code>[1] train-mlogloss:0.674595 test-mlogloss:0.684052 
[2] train-mlogloss:0.658466 test-mlogloss:0.676982 
[3] train-mlogloss:0.642685 test-mlogloss:0.667532 
[4] train-mlogloss:0.628284 test-mlogloss:0.660133 
[5] train-mlogloss:0.615050 test-mlogloss:0.653341 
[6] train-mlogloss:0.604056 test-mlogloss:0.645568 
[7] train-mlogloss:0.592582 test-mlogloss:0.640064 
[8] train-mlogloss:0.582170 test-mlogloss:0.637070 
[9] train-mlogloss:0.571289 test-mlogloss:0.634656 
[10]    train-mlogloss:0.561741 test-mlogloss:0.630252 
[11]    train-mlogloss:0.551731 test-mlogloss:0.628331 
[12]    train-mlogloss:0.542504 test-mlogloss:0.622866 
[13]    train-mlogloss:0.533755 test-mlogloss:0.618194 
[14]    train-mlogloss:0.525246 test-mlogloss:0.617723 
[15]    train-mlogloss:0.517447 test-mlogloss:0.613506 
[16]    train-mlogloss:0.509825 test-mlogloss:0.613574 
[17]    train-mlogloss:0.502859 test-mlogloss:0.609476 
[18]    train-mlogloss:0.496269 test-mlogloss:0.606218 
[19]    train-mlogloss:0.489355 test-mlogloss:0.606840 
[20]    train-mlogloss:0.483546 test-mlogloss:0.604335 </code></pre>
<pre class="r"><code>e &lt;- data.frame(bst_model$evaluation_log)
plot(e$iter, e$train_mlogloss, col = &#39;blue&#39;)
lines(e$iter, e$test_mlogloss, col = &#39;red&#39;)</code></pre>
<p><img src="/MLPost/2019-03-13-extreme-gradient-boosting-algorithm_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Now, that we have introduce the learning rate <a href="https://xgboost.readthedocs.io/en/latest/parameter.html"><strong>Eta</strong></a> the performance is better. </br> The range of Eta is between 0 and 1, we use <strong>eta = 0.05</strong> because with low eta the model is more robust to overfitting. </br> The <a href="https://xgboost.readthedocs.io/en/latest/parameter.html"><strong>learning rate Eta</strong></a> is the shrinkage you do at every step you are making. If you make 1 step at eta = 1.00, the step weight is 1.00. If you make 1 step at eta = 0.25, the step weight is 0.25. If our learning rate is 1.00, we will either land on 5 or 6 (in either 5 or 6 computation steps) which is not the optimum we are looking for. If our learning rate is 0.10, we will either land on 5.2 or 5.3 (in either 52 or 53 computation steps), which is better than the previous optimum. If our learning rate is 0.01, we will either land on 5.23 or 5.24 (in either 523 or 534 computation steps), which is again better than the previous optimum.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

