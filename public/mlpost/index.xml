<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MLPosts on Andrea Perlato</title>
    <link>/mlpost/</link>
    <description>Recent content in MLPosts on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/mlpost/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/mlpost/principal-component-analysis/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/principal-component-analysis/</guid>
      <description>body {text-align: justify}Principal Component Analysis PCA is a deterministic method (given an input will always produce the same output).  It is always good to perform a PCA: Principal Components Analysis (PCA) is a data reduction technique that transforms a larger number of correlated variables into a much smaller set of uncorrelated variables called PRINCIPAL COMPONENTS. For example, we might use PCA to transform many correlated (and possibly redundant) variables into a less number of uncorrelated variables that retain as much information from the original set of variables.</description>
    </item>
    
    <item>
      <title>Polynomial Regression &amp; Smoothing Splines</title>
      <link>/mlpost/polynomial-regression-smoothing-splines/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/polynomial-regression-smoothing-splines/</guid>
      <description>body {text-align: justify}Polynomial Linear Regression Polynomial Linear Regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, and has been used to describe nonlinear phenomena such as the progression of disease epidemics. Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function is linear in the unknown parameters that are estimated from the data.</description>
    </item>
    
    <item>
      <title>Fuzzy Matching Addresses to Prevent Fraudulent Application</title>
      <link>/mlpost/fuzzy-matching-addresses-to-prevent-fraudulent-application/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/fuzzy-matching-addresses-to-prevent-fraudulent-application/</guid>
      <description>body {text-align: justify}Application fraud refers to fraud committed by submitting a new credit application with fraudulent details to a credit provider. Normally, fraudsters collect the personal and financial data of innocent users from the identity documents, pay slips, bank statements, and other source documents to commit the application fraud. The information collected from all these documents will be either forged or sometimes the document itself will be stolen illegally or the details in the documents will be changed for the purpose of submitting a new credit application.</description>
    </item>
    
    <item>
      <title>Generalized Addictive Models GAMs</title>
      <link>/mlpost/generalized-addictive-models-gams/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/generalized-addictive-models-gams/</guid>
      <description>body {text-align: justify}Generalized Addictive Models GAMs incorporates non linear form of predictions, and are useful when we have not linearity between response variable and predictors. GAMs doesnâ€™t force the predictors to a square as in polynomial regression, but GAMes tries to do a smooth line. The data we use here is biocapacity of different countries.
library(psych)eco &amp;lt;- read.csv(&amp;quot;C:/07 - R Website/dataset/ML/biocap.csv&amp;quot;)pairs.panels(eco, method = &amp;quot;pearson&amp;quot;, # correlation methodhist.</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with LASSO Regression</title>
      <link>/mlpost/deal-multicollinearity-with-lasso-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-lasso-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with Ridge Regression</title>
      <link>/mlpost/deal-multicollinearity-with-ridge-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-ridge-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Outliers with Robust Regression</title>
      <link>/mlpost/deal-outliers-with-robust-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-outliers-with-robust-regression/</guid>
      <description>body {text-align: justify}This is a regression technique that can helps us alleviate the problem of outliers. Robust Regression is a family of regression techniques that is really quite immune to the presence of outliers. Least Trimmed Squares Regression is a technique that fit a regression function and is not effected by the presence of outliers. Least Trimmed Squares Regression attempts to minimise the sum of squared residuals over a subset of k points.</description>
    </item>
    
    <item>
      <title>Quantile Regression in Medical Expenditures</title>
      <link>/mlpost/quantile-regression-in-medical-expenditures/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/quantile-regression-in-medical-expenditures/</guid>
      <description>body {text-align: justify}The Quantile regression gives a more comprehensive picture of the effect of the independent variables on the dependent variable. Instead of estimating the model with average effects using the OLS linear model, the quantile regression produces different effects along the distribution (quantiles) of the dependent variable. The dependent variable is continuous with no zeros or too many repeated values.  Examples include estimating the effects of household income on food expenditures for low- and high-expenditure households, what are the factors influencing total medical expenditures for people with low, medium and high expenditures.</description>
    </item>
    
    <item>
      <title>Naive Bayes Classification</title>
      <link>/mlpost/naive-bayes-classification/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/naive-bayes-classification/</guid>
      <description>body {text-align: justify}Naive Bayes is an effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.
An intuitive explanation for the Maximum A Posteriori Probability MAP is to think probabilities as degrees of belief.</description>
    </item>
    
    <item>
      <title>Extreme Gradient Boosting Algorithm</title>
      <link>/mlpost/extreme-gradient-boosting-algorithm/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/extreme-gradient-boosting-algorithm/</guid>
      <description>body {text-align: justify}Extreme Gradient Boosting is extensively used because is fast and accurate, and can handle missing values.  Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.</description>
    </item>
    
    <item>
      <title>Customer segmentation via K-Means &amp; Hierarchical clustering</title>
      <link>/mlpost/customer-segmentation/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/customer-segmentation/</guid>
      <description>body {text-align: justify}Consider to have a big mall in a specific city that contains information of its clients that subcribed to a membership card. The last feature is Spending Score that is a score that the mall computed for each of their clients based on several criteria including for example their income and the number of times per week they show up in the mall and of course, the amount of dollars they spent in a year.</description>
    </item>
    
    <item>
      <title>Assessing the sucess of a new product via multiple classifiers</title>
      <link>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</guid>
      <description>body {text-align: justify}These are a series of analysis to illustate the main classification algorithms and their advantages. The table shows the business clients of a company that has just launched a new product online. Some of the clients responded positively to the ads by buying the product and other responded negatively by not buying the product. The last column of the table tells for each user if the user bought the product or not.</description>
    </item>
    
  </channel>
</rss>