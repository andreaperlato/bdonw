<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Naive Bayes Classification - Andrea Perlato</title>
<meta property="og:title" content="Naive Bayes Classification - Andrea Perlato">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Naive Bayes Classification</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>Naive Bayes is an effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"><strong>Maximum A Posteriori decision rule</strong></a> in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.</p>
<p>An intuitive explanation for the <strong>Maximum A Posteriori Probability MAP</strong> is to think probabilities as degrees of <strong>belief</strong>. For example, how likely are we vote for a candidate depends on our <strong>prior belief</strong>. We can modify our stand based on the <strong>evidence</strong>. Our final decision, based on evidence, is the <strong>posterior belief</strong>, which is what happens after we sifted through the evidence. </br> <strong>MPA</strong> is simply the maximum posterior belief: after going through all the debates, what is your most likely decision. </br></p>
<p>We use Naive Bayes in an example to predict, based on some features (e.g. the rank of the school student come from), if a student is admitted or rejected.</p>
<pre class="r"><code>library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
data &lt;- read.csv(&quot;C:/07 - R Website/dataset/ML/binary.csv&quot;)
data$rank &lt;- as.factor(data$rank)
data$admit &lt;- as.factor(data$admit)

# Visualization
pairs.panels(data[-1])</code></pre>
<p><img src="/MLPost/2019-03-14-naive-bayes-classification_files/figure-html/unnamed-chunk-1-1.png" width="80%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>data %&gt;%
  ggplot(aes(x=admit, y=gre, fill=admit)) + 
  geom_boxplot()</code></pre>
<p><img src="/MLPost/2019-03-14-naive-bayes-classification_files/figure-html/unnamed-chunk-1-2.png" width="80%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>data %&gt;%
  ggplot(aes(x=gre, fill=admit)) + 
  geom_density(alpha=0.8, color=&#39;black&#39;)</code></pre>
<p><img src="/MLPost/2019-03-14-naive-bayes-classification_files/figure-html/unnamed-chunk-1-3.png" width="80%" style="display: block; margin: auto;" /> In order to develop a model, we have to make sure that the independent variables are not highly correlated. </br> We can see from the scatterplot above that the only numerical variables are <strong>gre</strong> and <strong>gpa</strong>, and they are not strogly correlated (R-squared=0.38). Moreover, looking at the boxplot that compare <strong>admit</strong> as a function of <strong>gre</strong>, there is a significant overlap between the two levels of admit. From the dnsity plot of <strong>gre</strong> as a function on <strong>admit</strong>, we can see that students not admitted (admit=1) have higher <strong>gre</strong> comapared to student admitrìted (admit=0). Anyway, there is a significat amount of overlap between the two distributioins. </br> The same is for <strong>gpa</strong>, here not shown. </br></p>
<pre class="r"><code># Data Preparation
set.seed(1234)
ind &lt;- sample(2, nrow(data), replace=TRUE, prob = c(0.8, 0.2))
train &lt;-data[ind == 1,]
test &lt;-data[ind == 2,]</code></pre>
<p>Now, the probability of a student admitted given he belongs to rank one: p(admit=1|rank=1) is equal to: **p(admit=1)*p(rank=1|admit=1)/p(rank=1)**</p>
<pre class="r"><code># Naive Bayes Model
model &lt;- naive_bayes(admit ~ ., data = train)
model</code></pre>
<pre><code>================================ Naive Bayes ================================= 
Call: 
naive_bayes.formula(formula = admit ~ ., data = train)

A priori probabilities: 

        0         1 
0.6861538 0.3138462 

Tables: 
      
gre           0        1
  mean 578.6547 622.9412
  sd   116.3250 110.9240

      
gpa            0         1
  mean 3.3552466 3.5336275
  sd   0.3714542 0.3457057

    
rank          0          1
   1 0.10313901 0.24509804
   2 0.36771300 0.42156863
   3 0.33183857 0.24509804
   4 0.19730942 0.08823529</code></pre>
<pre class="r"><code>plot(model)</code></pre>
<p><img src="/MLPost/2019-03-14-naive-bayes-classification_files/figure-html/unnamed-chunk-3-1.png" width="80%" style="display: block; margin: auto;" /><img src="/MLPost/2019-03-14-naive-bayes-classification_files/figure-html/unnamed-chunk-3-2.png" width="80%" style="display: block; margin: auto;" /><img src="/MLPost/2019-03-14-naive-bayes-classification_files/figure-html/unnamed-chunk-3-3.png" width="80%" style="display: block; margin: auto;" /></p>
<p>From the result above, we have <strong>A Priori</strong> probabilities to be admitted of 0.31: only 31% of the students were admitted to the program. Moreover, for numerical variables (gre, gpa) we have mean and standard deviation, and for categorical variables (rank) we have probabilities. For example, p(rank=1|admit=0)=0.103 and p(rank=1|admit=1)=0.245, as we can see from the result table above. </br> We have also three plots of the model that represent the density for the numerical variables and the bars for the categorical variable. </br></p>
<pre class="r"><code># Prediction
p &lt;- predict(model, data=train, type=&quot;prob&quot;)
head(cbind(p, train))</code></pre>
<pre><code>          0         1 admit gre  gpa rank
1 0.8449088 0.1550912     0 380 3.61    3
2 0.6214983 0.3785017     1 660 3.67    3
3 0.2082304 0.7917696     1 800 4.00    1
4 0.8501030 0.1498970     1 640 3.19    4
6 0.6917580 0.3082420     1 760 3.00    2
7 0.6720365 0.3279635     1 560 2.98    1</code></pre>
<pre class="r"><code># Confusion Matrix
p1 &lt;- predict(model, data=train)
(tab1 &lt;- table(p1, train$admit))</code></pre>
<pre><code>   
p1    0   1
  0 196  69
  1  27  33</code></pre>
<pre class="r"><code># Missclassification
1 - sum(diag(tab1)) / sum(tab1)</code></pre>
<pre><code>[1] 0.2953846</code></pre>
<p>From the table above, we can see that the first student has a probability of 0.84 to not be ammitted. If fact, admit=0, and he has low value of gre=380 and he came from a low rank=3. From the confusin matrix, we calculated a <strong>misclassification</strong> of 29%. </br> In order to reduce the misclassification rate we can use the *<strong>Kernel</strong>. In fact, kernel based densities may perform better when numerical variables are not normally distributed.</p>
<pre class="r"><code># Naive Bayes Model
model &lt;- naive_bayes(admit ~ ., data = train, usekernel = TRUE)
p2 &lt;- predict(model, data=train)
tab2 &lt;- table(p2, train$admit) # confusion matrix
1 - sum(diag(tab2)) / sum(tab2) # missclasification</code></pre>
<pre><code>[1] 0.2738462</code></pre>
<p>In fact, as we can see above, introducing the kernel based dentities the misclassification is reduced (from 29% to 27%).</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

