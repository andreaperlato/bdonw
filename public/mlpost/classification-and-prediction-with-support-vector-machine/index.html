<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Classification and Prediction with Support Vector Machine - Andrea Perlato</title>
<meta property="og:title" content="Classification and Prediction with Support Vector Machine - Andrea Perlato">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Classification and Prediction with Support Vector Machine</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>Support Vector Machine SVM is a linear classifier. We can consider SVM for linearly separable binary sets. The goal is to design a hyperplane (is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes). </br> The hyperplane classifies all the training vectors in two classes. We can have many possible hyperplanes that are able to classify correctly all the elements in the feature set, but the best choice will be the hyperplane that leaves the Maximum Margin from both classes. With Margins we mean the distance between the hyperplane and the closest elements from the hyperplane.</p>
<pre class="r"><code>data(iris)
summary(iris)</code></pre>
<pre><code>  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300  
 Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
       Species  
 setosa    :50  
 versicolor:50  
 virginica :50  
                
                
                </code></pre>
<pre class="r"><code>head(iris)</code></pre>
<pre><code>  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa</code></pre>
<pre class="r"><code># library(ggplot2)
# qplot(Petal.Length, Petal.Width, data=iris, color = Species)</code></pre>
<p>We are using the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set"><strong>iris</strong></a> dataset with 4 numerical variables and 1 factor which has 3 levels as described above. We can also see that the numerical variables have different ranges, it is a good pratice to normalize the data. </br> We create classification machine learning model that help us to predict the correct species. From the graph above, we can see there is a separation based on the <strong>Species</strong>, for example <strong>setosa</strong> species is very far from the other two groups, and between <strong>versicolor</strong> and <strong>virginica</strong> there is a small overlap. </br></p>
<p>With <strong>Support Vector Machine SVM</strong> we are looking for optimal separating hyperplane between two classes. And to do that SMV maximize the margin around the hyperplane. The point that lie on the boundary ar called <strong>Support Vectors</strong>, and the middle line is the <strong>Seprarating Hyperplane</strong>. </br> In situatins where we are not able to obtain a linear separator, the data are projected into a higher dimentional space, so that, data points, can become linearly separable. </br> In this case, we use the the <strong>Kernel Trick</strong>, using the <strong>Gaussian Radial Basis Function</strong>.</p>
<pre class="r"><code>library(e1071)
mymodel &lt;- svm(Species~., data=iris)
summary(mymodel)</code></pre>
<pre><code>
Call:
svm(formula = Species ~ ., data = iris)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.25 

Number of Support Vectors:  51

 ( 8 22 21 )


Number of Classes:  3 

Levels: 
 setosa versicolor virginica</code></pre>
<pre class="r"><code># Plot two-dimensional projection of the data with highlighting classes and support vectors
# The Species classes are shown in different shadings
plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4)) # specify a list of named values for the dimensions held constant</code></pre>
<p><img src="/MLPost/2019-04-03-classification-and-prediction-with-support-vector-machine_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Confusion Matrix and Missclassification Error
pred &lt;- predict(mymodel, iris) 
tab &lt;- table(Predicted = pred, Actual = iris$Species)
tab</code></pre>
<pre><code>            Actual
Predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         48         2
  virginica       0          2        48</code></pre>
<pre class="r"><code># Missclassification Rate
1-sum(diag(tab))/sum(tab)</code></pre>
<pre><code>[1] 0.02666667</code></pre>
<p>As we can see from the result above, we use Gaussian Radial Basis Function, <strong>cost</strong> is the constaint violation. </br> The two-dimensional plot above, is a projection of the data with highlighting classes and support vectors. The <strong>Species</strong> classes are shown in different shadings. Inside the <strong>blue class setosa</strong> we have 8 points depicted with a cross, and these are the suppor vectors for <strong>setosa</strong>. Similarly, we have points depicted with red cross points for <strong>versicolor</strong>, and green cross points for <strong>virginica</strong>. </br></p>
<p>From the <strong>Confusion Matrix</strong> above, we have only 2 observation missclassified for <strong>versicolor</strong>, and 2 observation missclassified for <strong>virginica</strong>. </br> We have also a missclassification rate, of <strong>2.6%</strong>. </br> If we try to use SVM with a <strong>linear kernel</strong> (not shown here), instead of a SVM with a <strong>radial kernel</strong>, the missclassification rate is a bit higher.</p>
<pre class="r"><code>mymodel &lt;- svm(Species~., data=iris,
               kernel = &quot;polynomial&quot;)

plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4))</code></pre>
<p><img src="/MLPost/2019-04-03-classification-and-prediction-with-support-vector-machine_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>pred &lt;- predict(mymodel, iris) 
tab &lt;- table(Predicted = pred, Actual = iris$Species)

1-sum(diag(tab))/sum(tab)</code></pre>
<pre><code>[1] 0.04666667</code></pre>
<p>If we also try to use a SVM with a <strong>polynomial kernel</strong>, as we can see from the graph above, the missclassification rate is increased to <strong>4.6%</strong>. </br></p>
<p>We can try to tune the model in order to have better classification rate. Tune is also called hyperparameter optimization, and it helps to select the best model.</p>
<pre class="r"><code># Tuning
set.seed(123)

tmodel &lt;- tune(svm, Species~., data=iris,
               ranges = list(epsilon = seq(0,1,0.1), # sequence from 0 to 1 with an icrement of 0.1 
              cost = 2^(2:7)))                       # cost captures the cost of constant violatio
                                                     # if cost is too high, we have penalty for non-separable points, and the model store too many support vectors

plot(tmodel)</code></pre>
<p><img src="/MLPost/2019-04-03-classification-and-prediction-with-support-vector-machine_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We use <strong>epsilon</strong> and <strong>cost</strong> as tune paramentrs. </br> The <strong>cost</strong> parameter captures the cost of constant violatio. If <strong>cost is too high</strong>, we have penalty for non-separable points, and as a consequence we have a model that store too many support vectors, leading to <strong>overfitting</strong>. On the contrary, if <strong>cost is too small</strong>, we may end up with <strong>underfitting</strong>. </br></p>
<p>The value of <strong>epsilon</strong> defines a margin of tolerance where no penalty is given to errors. In fact, in SVM we can have <strong>hard</strong> or <strong>soft</strong> margins, where soft allow observations inside the margins. Soft margin is used when two classes are not linearly separable. </br></p>
<p>the plot here above gives us the performance evaluation of SMV for the <strong>epsilon</strong> and <strong>cost</strong> parameters. Darker regions means better results, and so lower misclassification error. By interpreting this graph we can choose the best model parameters.</p>
<pre class="r"><code>mymodel &lt;- tmodel$best.model
summary(mymodel)</code></pre>
<pre><code>
Call:
best.tune(method = svm, train.x = Species ~ ., data = iris, ranges = list(epsilon = seq(0, 
    1, 0.1), cost = 2^(2:7)))


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  8 
      gamma:  0.25 

Number of Support Vectors:  35

 ( 6 15 14 )


Number of Classes:  3 

Levels: 
 setosa versicolor virginica</code></pre>
<pre class="r"><code>plot(mymodel, data=iris, 
     Petal.Width~Petal.Length,
     slice = list(Sepal.Width=3, Sepal.Length=4))</code></pre>
<p><img src="/MLPost/2019-04-03-classification-and-prediction-with-support-vector-machine_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>pred &lt;- predict(mymodel, iris) 
tab &lt;- table(Predicted = pred, Actual = iris$Species)

1-sum(diag(tab))/sum(tab)</code></pre>
<pre><code>[1] 0.01333333</code></pre>
<p>Fomr the summary above, now we have <strong>35 support vectors</strong>: <strong>6</strong> for <strong>setosa</strong>, <strong>15</strong> for <strong>versicolor</strong>, and <strong>14</strong> for <strong>virginica</strong>. The graph here above expain the result obtained with the best model. Looking at the confusion matrix and missclassification error, we can see that only 2 observations are missclassified and the missclassification error is 1.3% which is significant less from what the got earlier.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

