<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Homepage on Andrea Perlato</title>
    <link>/</link>
    <description>Recent content in Homepage on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 May 2016 21:48:51 -0700</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stochastic Gradient Descent</title>
      <link>/aipost/stochastic-gradient-descent/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/stochastic-gradient-descent/</guid>
      <description>body { text-align: justify}  Is Gradient Descent, we used the sum of the squared residuals as the Loss Function to determine how well the initial line fit the data. Than, we took the derivative of the sum of the squared residuals with respect to the intercept and slope. We repeated that process a lot of times untill we took the maximum number of steps, or the spteps became very, very small.</description>
    </item>
    
    <item>
      <title>How to create 3D and 4D plot</title>
      <link>/graphpost/how-to-create-3d-and-4d-plot/</link>
      <pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/how-to-create-3d-and-4d-plot/</guid>
      <description>body {text-align: justify}A 3D plot is quite popular, in particular in business presentation, but it is almost always inappropriately used. In fact, it is rare to see a 3D plot that could not be improved by turning into a regular 2D figure.  Visualizations using 3D position scales can sometimes be appropriate, however. If the visualization is show it slowly rotation, rather than a static image from one prospective, will allow the viewer to discern where in 3D space different graphicla elements resides.</description>
    </item>
    
    <item>
      <title>How to compare data at different scales</title>
      <link>/graphpost/how-to-compare-data-at-different-scales/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/how-to-compare-data-at-different-scales/</guid>
      <description>body {text-align: justify}I recently underwent to different tests of the so called Hair Analysis, and as a result I was given different conclusions. More precisely, I sent two of my hair samples to a two different labs, one in Italy and the second in Switzerland. The results are quite different as described by the table and graph below.  In 2011 a comprehensive review was published of the scientific literature on hair elemental (mineral) analysis.</description>
    </item>
    
    <item>
      <title>The Learning Process</title>
      <link>/aipost/the-learning-process/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/the-learning-process/</guid>
      <description>body {text-align: justify}Learning means to generalize what we learned and improve the performance of the same task based on a given measure. More specifically, it means to adjusting the parameters of the model in order to accurately predict the dependent variables on new input data. More formally we can define two main functions: the Score Function and the Loss Function.  The score Function describes our mapping from the input space x to the output space y.</description>
    </item>
    
    <item>
      <title>Cox Proportional Hazards Model</title>
      <link>/tspost/cox-proportional-hazards-model/</link>
      <pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/cox-proportional-hazards-model/</guid>
      <description>body {text-align: justify}An hazard rate is the probability estimate of the time it takes for an event to take place. The event can be anything ranging from death of an organism or failure of a machine or any other time to event setting.  There are external factors that influence the probabililty of an event, covariates. For example: how many miles was the car used or did the owner exchange the oil regularly.</description>
    </item>
    
    <item>
      <title>Parametric Regression Model in Survival Analysis</title>
      <link>/tspost/parametric-regression-model-in-survival-analysis/</link>
      <pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/parametric-regression-model-in-survival-analysis/</guid>
      <description>body {text-align: justify}There are differences between Parametric Models (e.g. Kaplan-Meier), Semi-Parametric Models (e.g. Cox Proportional Hazard), and Non-Parametric Models.  The graph below gives the main pieces of information. A survival analysis can be defined as consisting of two parts: the core survial object with a time indicator plus the corresponding event status (used to calculate the baseline hazard). The second part of the survival model consists of the covariates.</description>
    </item>
    
    <item>
      <title>Survival Trees</title>
      <link>/tspost/survival-trees/</link>
      <pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/survival-trees/</guid>
      <description>body {text-align: justify}A survival tree is a decision tree fitted on the survival data. It allows covariates to be incorporated quite like in a Cox Proportional Hazard.  When we use a survival treee, we have to keep a few things in mind. First of all, it is a very good choice for huge dataset. Decision or survival trees require a huge amount of data to get precise enough.</description>
    </item>
    
    <item>
      <title>Survival Analysis: Kaplan-Meier &amp; Logrank test</title>
      <link>/tspost/introduction-of-survival-analysis/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/introduction-of-survival-analysis/</guid>
      <description>body {text-align: justify}The ultimate goal of survival analysis is to gain information on the expected duration of time untill one or even more events happen. Survival analysis is applied in different fields and most of these fields have different terms for the same concept. So, sometimes it is called Reliability Theory or Reliability Analysis in engineering. It is also called Duration Analysis in economics or Event-history Analysis is sociology.</description>
    </item>
    
    <item>
      <title>Time Series Classification</title>
      <link>/tspost/time-series-classification/</link>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/time-series-classification/</guid>
      <description>body {text-align: justify}In order to perform a Time series Classification we use Decision Tree, and then we look at the performance of the classification. 
We use the Synthetic Control Chart Time Series. This dataset contains 600 examples of control charts synthetically generated by the process in Alcock and Manolopoulos (1999).
data &amp;lt;- read.table(&amp;quot;C:/07 - R Website/dataset/TS/synthetic_control.txt&amp;quot;, header = FALSE)# Data Preparationpattern100 &amp;lt;- c(rep(&amp;#39;Normal&amp;#39;, 100),rep(&amp;#39;Cyclic&amp;#39;, 100),rep(&amp;#39;Increasing trend&amp;#39;, 100),rep(&amp;#39;Decreasing trend&amp;#39;, 100),rep(&amp;#39;Upward shift&amp;#39;, 100),rep(&amp;#39;Downward shift&amp;#39;, 100))# Create data framenewdata &amp;lt;- data.</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>/mlpost/linear-discriminant-analysis/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/linear-discriminant-analysis/</guid>
      <description>body {text-align: justify}Linear Discriminant Analysis was originally developed by R.A. Fisher to classify subjects into one of the two clearly defined groups. It was later expanded to classify subjects inoto more than two groups. It helps to find linear combination of original variables that provide the best possible separation between the groups.  Linear Discriminant Analysis is focused on maximizing the separability among known categories. The problem is when 2 features are not sufficient to capture the most of variation.</description>
    </item>
    
    <item>
      <title>Time Series Clustering</title>
      <link>/tspost/time-series-clustering/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/time-series-clustering/</guid>
      <description>body {text-align: justify}Clustering is the task of grouping s set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. Connectivity-based clustering connects objects to form clusters based on their distance. A cluster can be descibed by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using dendrogram.</description>
    </item>
    
    <item>
      <title>Classification and Prediction with Support Vector Machine</title>
      <link>/mlpost/classification-and-prediction-with-support-vector-machine/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/classification-and-prediction-with-support-vector-machine/</guid>
      <description>body {text-align: justify}Support Vector Machine SVM is a linear classifier. We can consider SVM for linearly separable binary sets. The goal is to design a hyperplane (is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes).  The hyperplane classifies all the training vectors in two classes. We can have many possible hyperplanes that are able to classify correctly all the elements in the feature set, but the best choice will be the hyperplane that leaves the Maximum Margin from both classes.</description>
    </item>
    
    <item>
      <title>Interactive Forecasting</title>
      <link>/graphpost/interactive-forecasting/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/interactive-forecasting/</guid>
      <description>body {text-align: justify}In this post, we use some fairly new technology of time series analysis namely neural nets and interactive charting tools.
INTERACTIVE GRAPH  The time series results should be presented interactively in order to highlight certain features.
# Handle outlierslibrary(forecast)myts &amp;lt;- tsclean(myts)# Set up a NNmynnetar &amp;lt;- nnetar(myts)# Forecast 3 yearsnnetforecast &amp;lt;- forecast(mynnetar, h = 36, PI = TRUE) # PI create the prediction intervals for the forecastlibrary(ggplot2)# Data we need for the graphdata &amp;lt;- nnetforecast$x # raw datalower &amp;lt;- nnetforecast$lower[,2] # confidence intervals lower boundupper &amp;lt;- nnetforecast$upper[,2] # confidence intervals upper boundpforecast &amp;lt;- nnetforecast$mean # th element meanmydata &amp;lt;- cbind(data, lower, upper, pforecast) # put everything in one dataframelibrary(dygraphs)dygraph(mydata, main = &amp;quot;Campsite Restaurant&amp;quot;) %&amp;gt;% # get data and the captiondyRangeSelector() %&amp;gt;% # the zoom tooldySeries(name = &amp;quot;data&amp;quot;, label = &amp;quot;Revenue Data&amp;quot;) %&amp;gt;% # add time series which are store in: data &amp;lt;- nnetforecast$xdySeries(c(&amp;quot;lower&amp;quot;,&amp;quot;pforecast&amp;quot;,&amp;quot;upper&amp;quot;), label = &amp;quot;Revenue Forecast&amp;quot;) %&amp;gt;% # add the forecast and CIdyLegend(show = &amp;quot;always&amp;quot;, hideOnMouseOut = FALSE) %&amp;gt;% # add the legend (time series + forecast)dyAxis(&amp;quot;y&amp;quot;, label = &amp;quot;Monthly Revenue USD&amp;quot;) %&amp;gt;% # label the y axisdyHighlight(highlightCircleSize = 5, # specify what happen when the mouse in hovering the graphhighlightSeriesOpts = list(strokeWidth = 2)) %&amp;gt;%dyOptions(axisLineColor = &amp;quot;navy&amp;quot;, gridLineColor = &amp;quot;grey&amp;quot;) %&amp;gt;% # set axis and fridline colorsdyAnnotation(&amp;quot;2010-8-1&amp;quot;, text = &amp;quot;CF&amp;quot;, tooltip = &amp;quot;Camp Festival&amp;quot;, attachAtBottom = T) # add annotation{&#34;</description>
    </item>
    
    <item>
      <title>Feature Selection using Boruta Algorithm</title>
      <link>/mlpost/feature-selection-using-boruta-algorithm/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/feature-selection-using-boruta-algorithm/</guid>
      <description>body {text-align: justify}Variable selection is an important aspect because it helps in building predictive models free from correlated variables, biases and unwanted noise.  The Boruta Algorithm is a feature selection algorithm. As a matter of interest, Boruta algorithm derive its name from a demon in Slavic mythology who lived in pine forests. 
How Boruta Algorithm works  Firstly, it adds randomness to the given data set by creating shuffled copies of all features which are called Shadow Features.</description>
    </item>
    
    <item>
      <title>Random Forest Hyperparameters Tuning</title>
      <link>/mlpost/random-forest-hyperparameters-tuning/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/random-forest-hyperparameters-tuning/</guid>
      <description>body {text-align: justify}Random Forest is a Bagging process of Ensemble Learners.  Random Forests are built from Decision Tree. Decision Trees work great, but they are not flexible when it comes to classify new samples. It creates a bootstrapped dataset with the same size of the original, and to do that Random Forest randomly selects rows with replacement. After creating a bootstrap dataset, it creates a decision tree using the bootstrapped dataset, but using only a subset of variables at each step.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/mlpost/principal-component-analysis/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/principal-component-analysis/</guid>
      <description>body {text-align: justify}Principal Component Analysis PCA is a deterministic method (given an input will always produce the same output).  It is always good to perform a PCA: Principal Components Analysis (PCA) is a data reduction technique that transforms a larger number of correlated variables into a much smaller set of uncorrelated variables called PRINCIPAL COMPONENTS. For example, we might use PCA to transform many correlated (and possibly redundant) variables into a less number of uncorrelated variables that retain as much information from the original set of variables.</description>
    </item>
    
    <item>
      <title>Interactive Dashboard</title>
      <link>/graphpost/interactive-dashboard/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/interactive-dashboard/</guid>
      <description>body {text-align: justify}A lot of the times when dashboards are implemented they are with a very specific dataset. The problem with this is that we have to rebuild them from scratch every time. The advantage to use shiny is the possibility to create interactive dashboard or webapp without, and reusing the code already wirtten we can adapt it with new data. Here below, there are some examples of interactive dashboards.</description>
    </item>
    
    <item>
      <title>Polynomial Regression &amp; Smoothing Splines</title>
      <link>/mlpost/polynomial-regression-smoothing-splines/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/polynomial-regression-smoothing-splines/</guid>
      <description>body {text-align: justify}Polynomial Linear Regression Polynomial Linear Regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, and has been used to describe nonlinear phenomena such as the progression of disease epidemics. Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function is linear in the unknown parameters that are estimated from the data.</description>
    </item>
    
    <item>
      <title>Fuzzy Matching Addresses to Prevent Fraudulent Application</title>
      <link>/mlpost/fuzzy-matching-addresses-to-prevent-fraudulent-application/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/fuzzy-matching-addresses-to-prevent-fraudulent-application/</guid>
      <description>body {text-align: justify}Application fraud refers to fraud committed by submitting a new credit application with fraudulent details to a credit provider. Normally, fraudsters collect the personal and financial data of innocent users from the identity documents, pay slips, bank statements, and other source documents to commit the application fraud. The information collected from all these documents will be either forged or sometimes the document itself will be stolen illegally or the details in the documents will be changed for the purpose of submitting a new credit application.</description>
    </item>
    
    <item>
      <title>Generalized Addictive Models GAMs</title>
      <link>/mlpost/generalized-addictive-models-gams/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/generalized-addictive-models-gams/</guid>
      <description>body {text-align: justify}Generalized Addictive Models GAMs incorporates non linear form of predictions, and are useful when we have not linearity between response variable and predictors. GAMs doesn’t force the predictors to a square as in polynomial regression, but GAMes tries to do a smooth line. The data we use here is biocapacity of different countries.
library(psych)eco &amp;lt;- read.csv(&amp;quot;C:/07 - R Website/dataset/ML/biocap.csv&amp;quot;)pairs.panels(eco, method = &amp;quot;pearson&amp;quot;, # correlation methodhist.</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with LASSO Regression</title>
      <link>/mlpost/deal-multicollinearity-with-lasso-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-lasso-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with Ridge Regression</title>
      <link>/mlpost/deal-multicollinearity-with-ridge-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-ridge-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Outliers with Robust Regression</title>
      <link>/mlpost/deal-outliers-with-robust-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-outliers-with-robust-regression/</guid>
      <description>body {text-align: justify}This is a regression technique that can helps us alleviate the problem of outliers. Robust Regression is a family of regression techniques that is really quite immune to the presence of outliers. Least Trimmed Squares Regression is a technique that fit a regression function and is not effected by the presence of outliers. Least Trimmed Squares Regression attempts to minimise the sum of squared residuals over a subset of k points.</description>
    </item>
    
    <item>
      <title>Quantile Regression in Medical Expenditures</title>
      <link>/mlpost/quantile-regression-in-medical-expenditures/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/quantile-regression-in-medical-expenditures/</guid>
      <description>body {text-align: justify}The Quantile regression gives a more comprehensive picture of the effect of the independent variables on the dependent variable. Instead of estimating the model with average effects using the OLS linear model, the quantile regression produces different effects along the distribution (quantiles) of the dependent variable. The dependent variable is continuous with no zeros or too many repeated values.  Examples include estimating the effects of household income on food expenditures for low- and high-expenditure households, what are the factors influencing total medical expenditures for people with low, medium and high expenditures.</description>
    </item>
    
    <item>
      <title>Interactive Graphs</title>
      <link>/graphpost/interactive-graphs/</link>
      <pubDate>Fri, 15 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/interactive-graphs/</guid>
      <description>body {text-align: justify}In this post we are going to create interactive graphs using Plotly.  Plotly allows us to create interactive charts, plot and maps with R. Plotly is designed to build a vast range of visualizations.  Crucially, it has the ability to automatically create interactive charts from the output ggplot2 which is the most abvanced R library to create scientific graphs.</description>
    </item>
    
    <item>
      <title>Interactive Tables</title>
      <link>/graphpost/interactive-tables/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/interactive-tables/</guid>
      <description>body {text-align: justify}Oftern, it is useful to provide interactive tables alonside charts. Responsive designed web content reflows itself dependent on the with of the browser window. There are many columns in the table which are over to the right-hand side and we need to scroll to access them.  So, could be really nice that the columns which don’t fit on the screen, are instead collapsed somehow, and optionally enable these.</description>
    </item>
    
    <item>
      <title>Naive Bayes Classification</title>
      <link>/mlpost/naive-bayes-classification/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/naive-bayes-classification/</guid>
      <description>body {text-align: justify}Naive Bayes is an effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.
An intuitive explanation for the Maximum A Posteriori Probability MAP is to think probabilities as degrees of belief.</description>
    </item>
    
    <item>
      <title>Extreme Gradient Boosting Algorithm</title>
      <link>/mlpost/extreme-gradient-boosting-algorithm/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/extreme-gradient-boosting-algorithm/</guid>
      <description>body {text-align: justify}Extreme Gradient Boosting is extensively used because is fast and accurate, and can handle missing values.  Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.</description>
    </item>
    
    <item>
      <title>Forecasting Product Demand</title>
      <link>/tspost/forecasting-product-demand/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/forecasting-product-demand/</guid>
      <description>body {text-align: justify}Accurately predicting demand for products allows a company to stay ahead of the market.  We will predict demand for multiple products across a region of a state in the US. Then we will roll up these predictions across many different regions of the same state to form a complete hierarchical forecasting system.  We need to forecast the future values of our data.</description>
    </item>
    
    <item>
      <title>Interactive Network</title>
      <link>/graphpost/interactive-network/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/interactive-network/</guid>
      <description>body {text-align: justify}Interactive Network is incredibly useful for visualizing the connections and relatioship between individuals, locations, and other data sets.
library(&amp;quot;tidyverse&amp;quot;)library(&amp;quot;leaflet&amp;quot;)library(&amp;quot;oidnChaRts&amp;quot;)transport_data &amp;lt;- read_csv(&amp;quot;C:/07 - R Website/dataset/Graph/transport_data.csv&amp;quot;)colnames(transport_data) &amp;lt;- colnames(transport_data) %&amp;gt;%gsub(&amp;quot;sender&amp;quot;, &amp;quot;start&amp;quot;, .) %&amp;gt;%gsub(&amp;quot;receiver&amp;quot;, &amp;quot;end&amp;quot;, .)transport_data &amp;lt;- transport_data %&amp;gt;%unite(start.location, c(start.country, start.city, start.state)) %&amp;gt;%unite(end.location, c(end.country, end.city, end.state))transport_data %&amp;gt;%geo_lines_plot(){&#34;</description>
    </item>
    
    <item>
      <title>Applied Time Series and Forecasting </title>
      <link>/tspost/applied-time-series-and-forecasting/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/applied-time-series-and-forecasting/</guid>
      <description>body {text-align: justify}In this particular field R is favored over Python. In fact, R has more features for Time Series.  A precious resource is the Rob Hyndman’s Blog. It explains step by step the standard univariate time series analysis.
FIRST  TRENDING DATA  In this example we explore how many people are working in a country: unemployment rate vs. labor force participation rate. That is used for propaganda purposes, because low unemployment rates show an optimistic picture about the economics of a country.</description>
    </item>
    
    <item>
      <title>Neural Nets and Interactive Graphs</title>
      <link>/tspost/neural-nets-and-interactive-graphs/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/neural-nets-and-interactive-graphs/</guid>
      <description>body {text-align: justify}In this post, we use some fairly new technology of time series analysis namely neural nets and interactive charting tools. These techniques are the state of the art. The dataset we use for this example has: missing data, outliers, poor formatting.  The dataset i about restaurant at a campsite that is open whole year. There is a peak season in summer, and so we aspect to have seasonal data and trend might be present.</description>
    </item>
    
    <item>
      <title>Supply Chain Foundations</title>
      <link>/tspost/supply-chain-foundations/</link>
      <pubDate>Mon, 25 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/supply-chain-foundations/</guid>
      <description>body { text-align: justify}  INTRODUCTION  Who do companies care about supply chain management? We know that food is always at the grocery store, and clothing at the department store. How they get there and who is making those items are there every single day? This is the job of supply chain manager. 
In supply chain management we start with purchasing, some people call it Procurement. The second part is Manufacturing and Operation, where the product is made, and we have to do that quickly and being able to do on a day.</description>
    </item>
    
    <item>
      <title>Gradient Descent Step by Step</title>
      <link>/theorypost/gradient-descent-step-by-step/</link>
      <pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/gradient-descent-step-by-step/</guid>
      <description>body {text-align: justify}INTRODUCTION  In statistics, Machine Learning and other Data Science fields, we optimize a lot of stuff. For example in linear regresion, we optimize the Intercept and Slope, or when we use Logistic Regression we optimize the squiggle. Moreover, in t-SNE we optimize clusters. The interesting thing is that gradient descent can optimize all these things and much more. A good example is the Sum of the Squared Residuals in Regression: in Machine Learning lingo this is a type of Loss Function.</description>
    </item>
    
    <item>
      <title>Data Science using Agile Methodology</title>
      <link>/theorypost/data-science-using-agile-methodology/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/data-science-using-agile-methodology/</guid>
      <description>body { text-align: justify}  INTRODUCTION  A data science team asks great questions, explores the data, and delivers key insights.  The best way to generate business value is to deliver a constant stream of key insights in short two-week sprints.  A short sprint will also help the team pivot so they can ask new questions based on what they learn from the data. 
WORK ON A DATA SCIENCE PROJECT  Typical project upfront requirements and we need to understand what we are going to build before to start the planning project.</description>
    </item>
    
    <item>
      <title>Statistical Background for Time Series</title>
      <link>/tspost/statistica-background-for-time-series/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/statistica-background-for-time-series/</guid>
      <description>body {text-align: justify}In this post we will review the statistical background for time series analysis and forecasting.  We start about how to compare different time seris models against each other.
Forecast Accuracy  It determine how much difference thare is between the actual value and the forecast for the value. The simplest way to m ake a comparison is via scale dependent error because all the models need to be on the same scale using the Mean Absolute Error - MAE and the Root Mean Squared Error - RMSE.</description>
    </item>
    
    <item>
      <title>Distinguish Benign and Malign Tumor via ANN</title>
      <link>/aipost/distinguish-benign-and-malign-tumor-via-ann/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/distinguish-benign-and-malign-tumor-via-ann/</guid>
      <description>body {text-align: justify}We try to recognize cancer in human breast using a multi-hidden layer artificial neural network via H2O package. We use the Wisconsin Breast-Cancer Dataset which is a collectioin of Dr.Wolberg real clinical cases. There are no images, but we can recognize malignal tumor based on 10 biomedical attributes. We have a total number of 699 patients divided in two classes: malignal and benign cancer.</description>
    </item>
    
    <item>
      <title>Event Processing</title>
      <link>/theorypost/event-processing/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/event-processing/</guid>
      <description>body {text-align: justify}Process Data  Event data consists of three basic components: the why, the what and the who.  Analysing event data is an iteractive process of three steps: extraction (from raw data to event log), processing (removing redundant details, enrich data by calculating variables) and analysis.  The analysis could be for instance which are the roles of different doctors and nurses organization and how they work together.</description>
    </item>
    
    <item>
      <title>Continuous Probability</title>
      <link>/theorypost/continuous-probability/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/continuous-probability/</guid>
      <description>body {text-align: justify}Empirical Cumulative Distribution Function  When summarizing a list of numeric values such as heights, it’s not useful to construct a distribution that assigns a proportion to each possible outcome. It is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the cumulative distribution function (CDF).  As an example, we define the empirical cumulative distribution function (eCDF) for heights for male adult students.</description>
    </item>
    
    <item>
      <title>How many Monte Carlo are enough?</title>
      <link>/theorypost/how-many-monte-carlo-are-enough/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/how-many-monte-carlo-are-enough/</guid>
      <description>body {text-align: justify}Here an example of the The birthday problem solution via Monte Carlo. Suppose you’re in a classroom with 22 people. If we assume this is a randomly selected group, what is the chance that at least two people have the same birthday? 
This is a problem of discrete probability. 
All right, first, note that birthdays can be represented as numbers between 1 and 365.</description>
    </item>
    
    <item>
      <title>The Monty Hall Problem</title>
      <link>/theorypost/the-monty-hall-problem/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/the-monty-hall-problem/</guid>
      <description>body {text-align: justify}In the 1970s, there was a game show called Let’s Make a Deal. Monty Hall was the hos, this is where the name of the problem comes from. At some point in the game, contestants were asked to pick one of three doors. Behind one door, there was a prize. The other two had a goat behind them. And this basically meant that you lost.</description>
    </item>
    
    <item>
      <title>Automotive Multivariate Visualization</title>
      <link>/graphpost/automotive-multivariate-visualization/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/graphpost/automotive-multivariate-visualization/</guid>
      <description>body {text-align: justify}This is a session dedicated to multivariate data visualization using some tipical feature of automobile. Here below we can see the matrix of correlation between features and a graphical representation.
 mpg cyl disp hp drat wt qsec vs am gear carbmpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.</description>
    </item>
    
    <item>
      <title>How Neural Network learn? An example of risk of churn.</title>
      <link>/aipost/how-neural-network-learn/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/how-neural-network-learn/</guid>
      <description>body {text-align: justify}Having a one layer neural network (single layer feedforeward) with the output value to be compare to the actual value. Baed on the activation function we have our output. In order to be able to lear, we have to compare the output value with the actual value via the cost funtion which is the half of the squred difference output and actual value.</description>
    </item>
    
    <item>
      <title>Customer segmentation via K-Means &amp; Hierarchical clustering</title>
      <link>/mlpost/customer-segmentation/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/customer-segmentation/</guid>
      <description>body {text-align: justify}Consider to have a big mall in a specific city that contains information of its clients that subcribed to a membership card. The last feature is Spending Score that is a score that the mall computed for each of their clients based on several criteria including for example their income and the number of times per week they show up in the mall and of course, the amount of dollars they spent in a year.</description>
    </item>
    
    <item>
      <title>Assessing the sucess of a new product via multiple classifiers</title>
      <link>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</guid>
      <description>body {text-align: justify}These are a series of analysis to illustate the main classification algorithms and their advantages. The table shows the business clients of a company that has just launched a new product online. Some of the clients responded positively to the ads by buying the product and other responded negatively by not buying the product. The last column of the table tells for each user if the user bought the product or not.</description>
    </item>
    
    <item>
      <title>Andrea Perlato</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>body { text-align: justify}  I started my caree as a Data Scientist more than 10 years ago in the field of Neuroscience at the University of Verona - School of Medicine, Italy. As point person in data analysis, I studied the single and multi-unit recordings from behaving non human primates. Moreover, I have been conducting psychophysic experiments on human subjects to investigate the brain plasticity in drug addicts.  Find out more.</description>
    </item>
    
  </channel>
</rss>