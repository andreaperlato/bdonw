<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Stochastic Gradient Descent - Andrea Perlato</title>
<meta property="og:title" content="Stochastic Gradient Descent - Andrea Perlato">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Stochastic Gradient Descent</h1>

    

    <div class="article-content">
      <style>
body {
text-align: justify}
</style>

<p>In <a href="https://andrea-perlato.netlify.com/theorypost/gradient-descent-step-by-step/"><strong>Gradient Descent</strong></a>, we used the sum of the squared residuals as the Loss Function to determine how well the initial line fit the data. Than, we took the derivative of the sum of the squared residuals with respect to the intercept and slope. We repeated that process a lot of times untill we took the maximum number of steps, or the spteps became very, very small.</p>

<p>Now, imag to have a Logistic Regression that used 10000 genes to predict if someone will have a disease. This means, we would have 10000 derivatives to plug the data into. so, for Big Data, Gradient Descent is too slow. This is where Stochastic Gradient Descent comes in handy. If we have 10000 samples, than Stochastic Gradient Descent would reduce the amount terms computed by a factor of 10000. It is specially iseful when there are redundancies in the data. For example, if we have 12 data points that form 3 clusters.
We start by picking a point randomly and we calculate the Height and Weight as presented in the example below.</p>

<p><center>
<img src="/img/randompoint.png" alt="" /></p>

<p></center></p>

<p>Just like with regular Gradient Descent, Stochastic Gradient Descent is sensitive to the value we choose for the Learning Rate.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    

    
  </body>
</html>

