<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.53" />


<title>Backpropagation Intuition - Andrea Perlato</title>
<meta property="og:title" content="Backpropagation Intuition - Andrea Perlato">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Backpropagation Intuition</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>The basic idea of backpropagation is to guess what the hidden units should look like based on what the input looks like and what the output should look like. The definition of backpropagation is a way of computing gradients through recursive application of chain rules. It is the standard way of computing gradients for ANNs. It is a very flexible solution for comupting the gradient through simple, incremental steps. </br> The first step is to understand what the <a href="https://en.wikipedia.org/wiki/Backpropagation"><strong>Chain Rule</strong></a> is all about. </br></p>
<p><strong>Chain Rule</strong> </br> The fundamental unit of every deep learning model is the Perceptron. The Perceptron algorithm was designed to classify visual inputs, categorizing subjects into one of two types and separating groups with a line: it is used for linearly-solvable problems. </br></p>
<p>In calculus, the Chain rule is a formula for computing the derivative of the composition of two or more functions called <a href="https://en.wikipedia.org/wiki/Perceptron"><strong>Perceptron</strong></a>. The Chain Rule allow us to break down a complex function in many smaller functions for which it is relatively straighforward to compute the derivative. Then, we simply chain them together by multiplication. </br></p>
<p>Let’s assume we have a set of neurons with weght as described by the figure below. </br></p>
<center>
<img src="/img/backpropagationweights.png" style="width:40.0%" />
</center>
<p>We can apply an <strong>Activation Function</strong> in order to have a smoother ranging from 0 to 1, for example the <strong>Sigmoid Function</strong> described by the formula below. </br></p>
<center>
<img src="/img/activationfunctionsigmoid.png" style="width:30.0%" />
</center>
<p>We want to compute the derivative with respect of the weight <strong>w0</strong> till to <strong>wn</strong>. First of all, we semplify the function introducing the <strong>g</strong> tems.</p>
<center>
<img src="/img/derivativeactivationfunction.jpg" style="width:35.0%" />
</center>
<p>Once we have the <strong>Gradient of the Loss Function</strong> with respect to <strong>All the Weights</strong> we can update our model simply by the function below, that make a subtraction of the gradient by alfa times which is the the learinign rate. Here, for semplicity, is not consider the <strong>Regularization term</strong>, but we have to remember to always take it into consideration, especially for large models. Of course, we don’t have to go through all this math each time we want to develop a new deep learning model, but we can use the fully-fledged packages.</p>
<p><strong>Hyper-parameters Optimization</strong> </br> Tillnow, we have seen how to Learn the Weights of the Perceptron on any Deep Learning Models. There are possibility to automatic tuning the hyper-parameters especially using <a href="https://en.wikipedia.org/wiki/Random_search"><strong>Random Search</strong></a> and <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization"><strong>Grid Search</strong></a>.</p>
<p>In the following example, we consider to tuning the hyper-parameter using the <a href="https://andrea-perlato.netlify.com/aipost/stochastic-gradient-descent/"><strong>Stochastic Gradient Descent</strong></a>. In general, evaluating the Loss Function with respect to the hyper-parameters means training the model from scratch each time which makes the problem computationally impossible to solve. That is why, it is pretty common to tune the hyper-parameters manually with some tricks such as <strong>Grid Search</strong>, <strong>Random Search</strong>, and <strong>Bayesian Methods</strong>.</p>
<center>
<img src="/img/gridrandomtuning.png" style="width:50.0%" />
</center>
<p>Let’s say we want to tune just two hyper-parameters, and each of the boxes represented by the figure above, represent the hyper-parameter space in which we can choose the hyper-parameter. </br> Since we can not test each possible combinations of th two hyper-parameters we can decide to cover the space with <strong>Discrete</strong> values in <strong>Grid Search</strong>, and <strong>Random Pairs</strong> in <strong>Random Search</strong>. It turns out that in general the Random Layout is better than the Grid Layout because for the hyper-parameters we can test more different values for the Important Parameters. </br> More pratically, we can implement a Random Search in R using <a href="https://en.wikipedia.org/wiki/H2O_(software)"><strong>H2O</strong></a> which come with a complete set of utility functions specifically designed for this purpose. The <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html"><strong>h2o.grid()</strong></a> function is very general and adapttive an helps to implement the Grid or Random Search with few lines of R code even for a comple hyper-parameters selection. </br></p>
<p>In the following example we use the Wisconsin Breast-Cancer Dataset which is a collectioin of Dr.Wolberg real clinical cases. There are no images, but we can recognize malignal tumor based on 10 biomedical attributes. We have a total number of 699 patients divided in two classes: malignal and benign cancer.</p>
<pre class="r"><code># Load libraries
library(mlbench)
library(h2o)

# General parameters
h2o.init()</code></pre>
<pre><code> Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         26 minutes 9 seconds 
    H2O cluster timezone:       Europe/Berlin 
    H2O data parsing timezone:  UTC 
    H2O cluster version:        3.20.0.8 
    H2O cluster version age:    8 months and 27 days !!! 
    H2O cluster name:           H2O_started_from_R_perlatoa_bbf562 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   2.63 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  4 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         AutoML, Algos, Core V3, Core V4 
    R Version:                  R version 3.5.1 (2018-07-02) </code></pre>
<pre class="r"><code>set.seed(123)

# Load the data
data(BreastCancer)

# Convert data for h2o
data &lt;- BreastCancer[, -1] # remove ID
data[, c(1:ncol(data))] &lt;- sapply(data[, c(1:ncol(data))], as.numeric) # interpret each features as numeric
data[, &#39;Class&#39;] &lt;- as.factor(data[, &#39;Class&#39;]) # interpret dependent variable as factor

# convert the dataset in three part in the h2o format
splitSample &lt;- sample(1:3, size=nrow(data), prob=c(0.7,0.15,0.15), replace=TRUE)
train_h2o &lt;- as.h2o(data[splitSample==1,])</code></pre>
<pre><code>
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>val_h2o &lt;- as.h2o(data[splitSample==2,])</code></pre>
<pre><code>
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre class="r"><code>test_h2o &lt;- as.h2o(data[splitSample==3,])</code></pre>
<pre><code>
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<p>Now, we can choose the best hyper-parameters using the <strong>Random Search</strong>. As we can see from the code below, we try to use many <strong>Activation Functions</strong>, many <strong>Hiden Layers with different units</strong>, different <strong>Drop-out Ratio</strong>, and <strong>L1 and L2 Regularization</strong>.</p>
<pre class="r"><code># Set hyper-parameters
hyper_params &lt;- list(
  activation =c(&quot;Rectifier&quot;, &quot;Tanh&quot;, &quot;Maxout&quot;,
                &quot;RectifierWithDropout&quot;, &quot;TanhWithDropout&quot;, &quot;MaxoutWithDropout&quot;),
                hidden = list(c(20,20), c(50,50), c(30,30,30), c(25,25,25,25)),
                input_dropout_ratio = c(0, 0.05),
                l1 = seq(0, 1e-4, 1e-6),
                l2 = seq(0, 1e-4, 1e-6))</code></pre>
<p>The Activation Functions used are: Rectifier, Tanh, Maxout, and we use different Drop-out Ration. The <strong>Drop-out</strong> process is about <a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning"><strong>Pruning</strong></a> some connections between neurons during training. It is very attractive to know that the <a href="https://en.wikipedia.org/wiki/Synaptic_pruning"><strong>human brain do the same pruning process in the early stage of the birth</strong></a>.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    

    
  </body>
</html>

