<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIPosts on Andrea Perlato</title>
    <link>/aipost/</link>
    <description>Recent content in AIPosts on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/aipost/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Alpha Beta Pruning</title>
      <link>/aipost/alpha-beta-pruning/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/alpha-beta-pruning/</guid>
      <description>body {text-align: justify}Introduction  A fascinating aspect of our brain is the Synaptic pruning. One of the grand strategies nature uses to construct nervous systems is to overproduce neural elements, such as neurons, axons and synapses, and then prune the excess. In fact, this overproduction is so substantial that only about half of the neurons mammalian embryos generate will survive until birth. At the same way the pruning in ANN is used to eliminate redundant connections between neurons during the training.</description>
    </item>
    
    <item>
      <title>Introduction to Convolutional Neural Networks</title>
      <link>/aipost/introduction-to-convolutional-neural-networks/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/introduction-to-convolutional-neural-networks/</guid>
      <description>body {text-align: justify}Convolutional Neural Networks is one of the most succesfully and used Neural Network Algorithm. Artificial Neural Network receive an input (single vector), and transform it through a series of hidden layers.</description>
    </item>
    
    <item>
      <title>Backpropagation Intuition</title>
      <link>/aipost/backpropagation/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/backpropagation/</guid>
      <description>body {text-align: justify}Introduction  We already know that there is in ANN a Forwward Propagation where the information is entered into the input layer, and then it is propagated forward to get our output values to compare with the actual values that we have in our training set, and then we calculate the errors. Then the errors are back propagated through the network in the opposite direction in order to adjust the weights.</description>
    </item>
    
    <item>
      <title>Stochastic Gradient Descent</title>
      <link>/aipost/stochastic-gradient-descent/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/stochastic-gradient-descent/</guid>
      <description>body { text-align: justify}  In Gradient Descent, we used the sum of the squared residuals as the Loss Function to determine how well the initial line fit the data. Than, we took the derivative of the sum of the squared residuals with respect to the intercept and slope. We repeated that process a lot of times untill we took the maximum number of steps, or the spteps became very, very small.</description>
    </item>
    
    <item>
      <title>The Learning Process</title>
      <link>/aipost/the-learning-process/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/the-learning-process/</guid>
      <description>body {text-align: justify}Learning means to generalize what we learned and improve the performance of the same task based on a given measure. More specifically, it means to adjusting the parameters of the model in order to accurately predict the dependent variables on new input data. More formally we can define two main functions: the Score Function and the Loss Function.  The score Function describes our mapping from the input space x to the output space y.</description>
    </item>
    
    <item>
      <title>Distinguish Benign and Malign Tumor via ANN</title>
      <link>/aipost/distinguish-benign-and-malign-tumor-via-ann/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/distinguish-benign-and-malign-tumor-via-ann/</guid>
      <description>body {text-align: justify}We try to recognize cancer in human breast using a multi-hidden layer artificial neural network via H2O package. We use the Wisconsin Breast-Cancer Dataset which is a collectioin of Dr.Wolberg real clinical cases. There are no images, but we can recognize malignal tumor based on 10 biomedical attributes. We have a total number of 699 patients divided in two classes: malignal and benign cancer.</description>
    </item>
    
    <item>
      <title>How Neural Network learn? An example of risk of churn.</title>
      <link>/aipost/how-neural-network-learn/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/how-neural-network-learn/</guid>
      <description>body {text-align: justify}Having a one layer neural network (single layer feedforeward) with the output value to be compare to the actual value. Baed on the activation function we have our output. In order to be able to lear, we have to compare the output value with the actual value via the cost funtion which is the half of the squred difference output and actual value.</description>
    </item>
    
  </channel>
</rss>