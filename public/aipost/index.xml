<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIPosts on Andrea Perlato</title>
    <link>/aipost/</link>
    <description>Recent content in AIPosts on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/aipost/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Backpropagation Intuition</title>
      <link>/aipost/backpropagation/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/backpropagation/</guid>
      <description>body {text-align: justify}The basic idea of backpropagation is to guess what the hidden units should look like based on what the input looks like and what the output should look like. The definition of backpropagation is a way of computing gradients through recursive application of chain rules. It is the standard way pf computing gradients for ANNs. It is a very flexible solution for comupting the gradient through simple, incremental steps.</description>
    </item>
    
    <item>
      <title>Stochastic Gradient Descent</title>
      <link>/aipost/stochastic-gradient-descent/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/stochastic-gradient-descent/</guid>
      <description>body { text-align: justify}  In Gradient Descent, we used the sum of the squared residuals as the Loss Function to determine how well the initial line fit the data. Than, we took the derivative of the sum of the squared residuals with respect to the intercept and slope. We repeated that process a lot of times untill we took the maximum number of steps, or the spteps became very, very small.</description>
    </item>
    
    <item>
      <title>The Learning Process</title>
      <link>/aipost/the-learning-process/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/the-learning-process/</guid>
      <description>body {text-align: justify}Learning means to generalize what we learned and improve the performance of the same task based on a given measure. More specifically, it means to adjusting the parameters of the model in order to accurately predict the dependent variables on new input data. More formally we can define two main functions: the Score Function and the Loss Function.  The score Function describes our mapping from the input space x to the output space y.</description>
    </item>
    
    <item>
      <title>Distinguish Benign and Malign Tumor via ANN</title>
      <link>/aipost/distinguish-benign-and-malign-tumor-via-ann/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/distinguish-benign-and-malign-tumor-via-ann/</guid>
      <description>body {text-align: justify}We try to recognize cancer in human breast using a multi-hidden layer artificial neural network via H2O package. We use the Wisconsin Breast-Cancer Dataset which is a collectioin of Dr.Wolberg real clinical cases. There are no images, but we can recognize malignal tumor based on 10 biomedical attributes. We have a total number of 699 patients divided in two classes: malignal and benign cancer.</description>
    </item>
    
    <item>
      <title>How Neural Network learn? An example of risk of churn.</title>
      <link>/aipost/how-neural-network-learn/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/how-neural-network-learn/</guid>
      <description>body {text-align: justify}Having a one layer neural network (single layer feedforeward) with the output value to be compare to the actual value. Baed on the activation function we have our output. In order to be able to lear, we have to compare the output value with the actual value via the cost funtion which is the half of the squred difference output and actual value.</description>
    </item>
    
  </channel>
</rss>