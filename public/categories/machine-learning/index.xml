<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Andrea Perlato</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Mar 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Generalized Addictive Models GAMs</title>
      <link>/mlpost/generalized-addictive-models-gams/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/generalized-addictive-models-gams/</guid>
      <description>body {text-align: justify}Generalized Addictive Models GAMs incorporates non linear form of predictions, and are useful when we have not linearity between response variable and predictors. GAMs doesnâ€™t force the predictors to a square as in polynomial regression, but GAMes tries to do a smooth line. The data we use here is biocapacity of different countries.
library(psych)eco &amp;lt;- read.csv(&amp;quot;C:/07 - R Website/dataset/ML/biocap.csv&amp;quot;)pairs.panels(eco, method = &amp;quot;pearson&amp;quot;, # correlation methodhist.</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with LASSO Regression</title>
      <link>/mlpost/deal-multicollinearity-with-lasso-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-lasso-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with Ridge Regression</title>
      <link>/mlpost/deal-multicollinearity-with-ridge-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-ridge-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Outliers with Robust Regression</title>
      <link>/mlpost/deal-outliers-with-robust-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-outliers-with-robust-regression/</guid>
      <description>body {text-align: justify}This is a regression technique that can helps us alleviate the problem of outliers. Robust Regression is a family of regression techniques that is really quite immune to the presence of outliers. Least Trimmed Squares Regression is a technique that fit a regression function and is not effected by the presence of outliers. Least Trimmed Squares Regression attempts to minimise the sum of squared residuals over a subset of k points.</description>
    </item>
    
    <item>
      <title>Quantile Regression in Medical Expenditures</title>
      <link>/mlpost/quantile-regression-in-medical-expenditures/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/quantile-regression-in-medical-expenditures/</guid>
      <description>body {text-align: justify}The Quantile regression gives a more comprehensive picture of the effect of the independent variables on the dependent variable. Instead of estimating the model with average effects using the OLS linear model, the quantile regression produces different effects along the distribution (quantiles) of the dependent variable. The dependent variable is continuous with no zeros or too many repeated values.  Examples include estimating the effects of household income on food expenditures for low- and high-expenditure households, what are the factors influencing total medical expenditures for people with low, medium and high expenditures.</description>
    </item>
    
    <item>
      <title>Naive Bayes Classification</title>
      <link>/mlpost/naive-bayes-classification/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/naive-bayes-classification/</guid>
      <description>body {text-align: justify}Naive Bayes is an effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.
An intuitive explanation for the Maximum A Posteriori Probability MAP is to think probabilities as degrees of belief.</description>
    </item>
    
    <item>
      <title>Extreme Gradient Boosting Algorithm</title>
      <link>/mlpost/extreme-gradient-boosting-algorithm/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/extreme-gradient-boosting-algorithm/</guid>
      <description>body {text-align: justify}Extreme Gradient Boosting is extensively used because is fast and accurate, and can handle missing values.  Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.</description>
    </item>
    
    <item>
      <title>Customer segmentation via K-Means &amp; Hierarchical clustering</title>
      <link>/mlpost/customer-segmentation/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/customer-segmentation/</guid>
      <description>body {text-align: justify}Consider to have a big mall in a specific city that contains information of its clients that subcribed to a membership card. The last feature is Spending Score that is a score that the mall computed for each of their clients based on several criteria including for example their income and the number of times per week they show up in the mall and of course, the amount of dollars they spent in a year.</description>
    </item>
    
    <item>
      <title>Assessing the sucess of a new product via multiple classifiers</title>
      <link>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</guid>
      <description>body {text-align: justify}These are a series of analysis to illustate the main classification algorithms and their advantages. The table shows the business clients of a company that has just launched a new product online. Some of the clients responded positively to the ads by buying the product and other responded negatively by not buying the product. The last column of the table tells for each user if the user bought the product or not.</description>
    </item>
    
  </channel>
</rss>