<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Artificial Intelligence on Andrea Perlato</title>
    <link>/categories/artificial-intelligence/</link>
    <description>Recent content in Artificial Intelligence on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stochastic Gradient Descent</title>
      <link>/aipost/stochastic-gradient-descent/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/stochastic-gradient-descent/</guid>
      <description>body { text-align: justify}  In Gradient Descent, we used the sum of the squared residuals as the Loss Function to determine how well the initial line fit the data. Than, we took the derivative of the sum of the squared residuals with respect to the intercept and slope. We repeated that process a lot of times untill we took the maximum number of steps, or the spteps became very, very small.</description>
    </item>
    
    <item>
      <title>The Learning Process</title>
      <link>/aipost/the-learning-process/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/the-learning-process/</guid>
      <description>body {text-align: justify}Learning means to generalize what we learned and improve the performance of the same task based on a given measure. More specifically, it means to adjusting the parameters of the model in order to accurately predict the dependent variables on new input data. More formally we can define two main functions: the Score Function and the Loss Function.  The score Function describes our mapping from the input space x to the output space y.</description>
    </item>
    
    <item>
      <title>Neural Nets and Interactive Graphs</title>
      <link>/tspost/neural-nets-and-interactive-graphs/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/neural-nets-and-interactive-graphs/</guid>
      <description>body {text-align: justify}In this post, we use some fairly new technology of time series analysis namely neural nets and interactive charting tools. These techniques are the state of the art. The dataset we use for this example has: missing data, outliers, poor formatting.  The dataset i about restaurant at a campsite that is open whole year. There is a peak season in summer, and so we aspect to have seasonal data and trend might be present.</description>
    </item>
    
    <item>
      <title>Distinguish Benign and Malign Tumor via ANN</title>
      <link>/aipost/distinguish-benign-and-malign-tumor-via-ann/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/distinguish-benign-and-malign-tumor-via-ann/</guid>
      <description>body {text-align: justify}We try to recognize cancer in human breast using a multi-hidden layer artificial neural network via H2O package. We use the Wisconsin Breast-Cancer Dataset which is a collectioin of Dr.Wolberg real clinical cases. There are no images, but we can recognize malignal tumor based on 10 biomedical attributes. We have a total number of 699 patients divided in two classes: malignal and benign cancer.</description>
    </item>
    
    <item>
      <title>How Neural Network learn? An example of risk of churn.</title>
      <link>/aipost/how-neural-network-learn/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/how-neural-network-learn/</guid>
      <description>body {text-align: justify}Having a one layer neural network (single layer feedforeward) with the output value to be compare to the actual value. Baed on the activation function we have our output. In order to be able to lear, we have to compare the output value with the actual value via the cost funtion which is the half of the squred difference output and actual value.</description>
    </item>
    
  </channel>
</rss>